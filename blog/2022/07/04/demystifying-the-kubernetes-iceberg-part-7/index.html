<!doctype html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.101.0"><link rel=canonical href=https://asankov.dev/blog/2022/07/04/demystifying-the-kubernetes-iceberg-part-7/><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Demystifying the Kubernetes Iceberg: Part 7 | Anton Sankov's Blog</title><meta name=description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta property="og:title" content="Demystifying the Kubernetes Iceberg: Part 7"><meta property="og:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta property="og:type" content="article"><meta property="og:url" content="https://asankov.dev/blog/2022/07/04/demystifying-the-kubernetes-iceberg-part-7/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-07-04T00:00:00+00:00"><meta property="article:modified_time" content="2022-07-04T19:06:50+03:00"><meta property="og:site_name" content="Anton Sankov's Blog"><meta itemprop=name content="Demystifying the Kubernetes Iceberg: Part 7"><meta itemprop=description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta itemprop=datePublished content="2022-07-04T00:00:00+00:00"><meta itemprop=dateModified content="2022-07-04T19:06:50+03:00"><meta itemprop=wordCount content="2276"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Demystifying the Kubernetes Iceberg: Part 7"><meta name=twitter:description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><link rel=preload href=/scss/main.min.2022a913c80b8b832ae4a6e4e95fb0a9f79c4a67af49994e93727f83e11d0b33.css as=style><link href=/scss/main.min.2022a913c80b8b832ae4a6e4e95fb0a9f79c4a67af49994e93727f83e11d0b33.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.5.1.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7Y9WXX1393"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7Y9WXX1393",{anonymize_ip:!1})}</script></head><body class="td-page td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar" style=position:relative><a class=navbar-brand href=/><span class=navbar-logo></span><span class="text-uppercase font-weight-bold text-nord5">Anton Sankov's Blog</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/blog/><span class=active>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About Me</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-docs-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-blog-li><a href=/blog/ title="Anton Sankov's Blog" class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-blog><span>Blog</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220704demystifying-the-kubernetes-iceberg-part-7-li><a href=/blog/2022/07/04/demystifying-the-kubernetes-iceberg-part-7/ class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id=m-blog20220704demystifying-the-kubernetes-iceberg-part-7><span class=td-sidebar-nav-active-item>Demystifying the Kubernetes Iceberg: Part 7</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220627demystifying-the-kubernetes-iceberg-part-6-li><a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220627demystifying-the-kubernetes-iceberg-part-6><span>Demystifying the Kubernetes Iceberg: Part 6</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220612demystifying-the-kubernetes-iceberg-part-5-li><a href=/blog/2022/06/12/demystifying-the-kubernetes-iceberg-part-5/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220612demystifying-the-kubernetes-iceberg-part-5><span>Demystifying the Kubernetes Iceberg: Part 5</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220605demystifying-the-kubernetes-iceberg-part-4-li><a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220605demystifying-the-kubernetes-iceberg-part-4><span>Demystifying the Kubernetes Iceberg: Part 4</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220529demystifying-the-kubernetes-iceberg-part-3-li><a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220529demystifying-the-kubernetes-iceberg-part-3><span>Demystifying the Kubernetes Iceberg: Part 3</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220522demystifying-the-kubernetes-iceberg-part-2-li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-2/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220522demystifying-the-kubernetes-iceberg-part-2><span>Demystifying the Kubernetes Iceberg: Part 2</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220515demystifying-the-kubernetes-iceberg-part-1-li><a href=/blog/2022/05/15/demystifying-the-kubernetes-iceberg-part-1/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220515demystifying-the-kubernetes-iceberg-part-1><span>Demystifying the Kubernetes Iceberg: Part 1</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220421securing-kubernetes-with-open-policy-agent-li><a href=/blog/2022/04/21/securing-kubernetes-with-open-policy-agent/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220421securing-kubernetes-with-open-policy-agent><span>Securing Kubernetes with Open Policy Agent</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220129different-ways-to-initialize-go-structs-li><a href=/blog/2022/01/29/different-ways-to-initialize-go-structs/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220129different-ways-to-initialize-go-structs><span>Different Ways to Initialize Go structs</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#tier-7>Tier 7</a><ul><li><a href=#nodes-os-configuration>Node&rsquo;s OS configuration</a></li><li><a href=#spot-instances>Spot instances</a></li><li><a href=#cloud-provider-integration>Cloud Provider Integration</a></li><li><a href=#node-rolling-updates>Node Rolling Updates</a></li><li><a href=#csi>CSI</a></li><li><a href=#cluster-api>Cluster API</a></li><li><a href=#security-sandboxing>security sandboxing</a></li><li><a href=#gvisor>gVisor</a></li><li><a href=#kata-containers>Kata containers</a></li><li><a href=#managing-instance-groups>Managing Instance Groups</a></li><li><a href=#os-level-node-bootstrap>OS level Node bootstrap</a></li><li><a href=#chaos-testingengineering>Chaos testing/engineering</a></li><li><a href=#cri>CRI</a></li><li><a href=#advanced-scheduling>Advanced scheduling</a></li><li><a href=#metallb>MetalLB</a></li><li><a href=#keepalived>Keepalived</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div><div class="taxonomy taxonomy-terms-cloud taxo-categories"><h5 class=taxonomy-title>Categories</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://asankov.dev/categories/gatekeeper/ data-taxonomy-term=gatekeeper><span class=taxonomy-label>Gatekeeper</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/go/ data-taxonomy-term=go><span class=taxonomy-label>Go</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/kubernetes/ data-taxonomy-term=kubernetes><span class=taxonomy-label>Kubernetes</span><span class=taxonomy-count>8</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/open-policy-agent/ data-taxonomy-term=open-policy-agent><span class=taxonomy-label>Open Policy Agent</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/security/ data-taxonomy-term=security><span class=taxonomy-label>Security</span><span class=taxonomy-count>2</span></a></li></ul></div></aside><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><div class=td-content><h1>Demystifying the Kubernetes Iceberg: Part 7</h1><div class=lead>Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &ldquo;Kubernetes Iceberg&rdquo; diagram by Flant.</div><div class="td-byline mb-4"><time datetime=2022-07-04 class=text-muted>Monday, July 04, 2022</time></div><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-categories"><h5 class=taxonomy-title>Categories:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://asankov.dev/categories/kubernetes/ data-taxonomy-term=kubernetes><span class=taxonomy-label>Kubernetes</span></a></li></ul></div><p class=reading-time><i class="fa fa-clock" aria-hidden=true></i>&nbsp; 11 minute read &nbsp;</p></header><p>This is the seventh article of the &ldquo;Demystifying the Kubernetes Iceberg&rdquo; series.
My goal for this series is to explain all concepts mentioned in the “Kubernetes Iceberg” diagram by <a href=https://flant.com/>Flant</a>.</p><p>This is the iceberg:</p><p><img src=/images/kubernetes-iceberg.png alt="The Kubernetes Iceberg meme"></p><p>In this article, we will continue with the second-to-last Tier 7 of the iceberg.</p><p>You can find the others articles here:</p><ul><li><a href=/blog/2022/05/15/demystifying-the-kubernetes-iceberg-part-1/>Part 1</a></li><li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-2/>Part 2</a></li><li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-3/>Part 3</a></li><li><a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/>Part 4</a></li><li><a href=/blog/2022/06/12/demystifying-the-kubernetes-iceberg-part-5/>Part 5</a></li><li><a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/>Part 6</a></li></ul><p>I will publish one article each week until I complete the whole iceberg.</p><h2 id=tier-7>Tier 7</h2><h3 id=nodes-os-configuration>Node&rsquo;s OS configuration</h3><p>Kubernetes Nodes are virtual or physical machines that run an OS (typically Linux).</p><p>Kubernetes manages the Nodes and assigns Pods to them if they are Healthy.</p><p>A Kubernetes Node can be configured the same way you can configure any other machine.</p><p>Different Kubernetes providers provide you different ways of configuration.</p><p>Most providers (<a href=https://cloud.google.com/kubernetes-engine/docs/how-to/node-system-config>GKE</a>, <a href=https://docs.microsoft.com/en-us/azure/aks/custom-node-configuration>AKS</a>) allow you to create config files which you use to create a Node.
These config files contain various stuff such as CPU limits, file handling limits, OS threshold, etc.</p><h3 id=spot-instances>Spot instances</h3><p>Spot instances are a cost-effective feature provided by cloud providers.
It allows you to request unused virtual machine instances at a lower price.</p><p>You can set the maximum price you are ready to pay for a VM; if such is available, one will be provision for you.
If the price increases, the virtual machine will be stopped and assigned to someone else.</p><p>Using spot instances must be done with caution since you can lose the instance anytime.</p><h4 id=spot-instances-and-kubernetes>Spot instances and Kubernetes</h4><p>While not entirely reliable for production workloads, spot instances can work quite well with Kubernetes.</p><p>That is so because Kubernetes is best-suited for elastic, stateless workloads that can easily be rescheduled on different nodes if one goes down or scaled in and out.</p><p><a href=https://aws.amazon.com/getting-started/hands-on/amazon-eks-with-spot-instances/>Some cloud providers</a> provide integration between spot instances and their Kubernetes offering.
It allows you to run some part of your Kubernetes cluster on top of spot instances and rebalance/reschedule workloads once a spot instance goes down.</p><p>This can be a more cost-effective way of running Kubernetes.</p><h3 id=cloud-provider-integration>Cloud Provider Integration</h3><p>Using Kubernetes managed by a cloud provider will give you a lot of the other functionalities of that cloud provider integrated into your Kubernetes by default.</p><p>This is done as a selling point by the cloud providers, e.g. they provide you with more feature out-of-the-box.
Also, it&rsquo;s another way to lock you into their system (by providing you proprietary services on top of an open platform like Kubernetes).</p><p>One example for a Cloud Provider Integration is the Load Balancer Service.
One you create a <code>Service</code> with <code>type: LoadBalancer</code> Kubernetes will provision a Load Balancer in the cloud that the cluster is running in.
This is not possible with out-of-the-shelf Kubernetes that runs on-prem, unless you install and configure a controller that know how to provision a Load Balancer for this environment.</p><p><strong>NOTE:</strong> This does lead to vendor lock-in, because the LoadBalancer Service is an abstract construct, and creating the same resource in a different cloud would have the same result - a Load Balancer will be provisioned for you.</p><h3 id=node-rolling-updates>Node Rolling Updates</h3><p>A rolling update means having multiple instances of a workload, and updating them one by one, so that always at least one instances of a workloads is running, and there is no downtime.</p><p>The same is valid for updating the Kubernetes cluster itself.
You can update the nodes one by one, with the latest Kubernetes components, without causing downtime to your cluster.</p><h3 id=csi>CSI</h3><p><a href=https://github.com/container-storage-interface/spec/blob/master/spec.md>CSI</a> stands for <strong>Container Storage Interface</strong>.</p><p>Its goal is to provide a unified interface for storage vendors that can write plugins that will work with each container orchestrator.</p><p>Kubernetes implements this interface as alpha in <a href=https://kubernetes.io/blog/2018/01/introducing-container-storage-interface/>Kubernetes 1.9</a> and GA in <a href=https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/>Kubernetes 1.13</a>.</p><p>This allows consumers to consume each storage vendor that has a CSI-compliant plugin.
Before Kubernetes implemented this, the code that provisions storage was in-tree, and introducing a new storage type or even fixing a bug in an existing one was slow and hard and had to be aligned with the Kubernetes release process.</p><h3 id=cluster-api>Cluster API</h3><p><a href=https://cluster-api.sigs.k8s.io/>Cluster API</a> is a project by <a href=https://github.com/kubernetes/community/tree/master/sig-cluster-lifecycle#readme>SIG Cluster Lifecycle</a> that aims to unify the process of provisioning and managing clusters, virtual machines, load balancers and everything else you need to run Kubernetes.</p><p>The need for something like this arose from the fact that there are <a href=https://www.cncf.io/certification/software-conformance/>100s of Kubernetes distributions and installers</a>, but no unified way of how they work.
Cluster API serves as an abstraction on top of them.
It can be implemented by anyone in whatever way they want.</p><p>For example, most cloud providers have implementations of Cluster API that provision and manage clusters in the respective cloud.</p><h3 id=security-sandboxing>security sandboxing</h3><p>Security sandboxing is yet another layer of isolation, on top of the existing ones.</p><p>It is done via additional hardening of the Pods runtime, isolating the Pods from one another even better than before.</p><p>It is enabled by secure runtimes such as <a href=#gvisor>gVisor</a> and <a href=#kata-containers>Kata</a> containers.</p><h3 id=gvisor>gVisor</h3><p><a href=https://gvisor.dev/docs/>gVisor</a> is an application kernel, written in Go, that implements a substantial portion of the <a href=https://en.wikipedia.org/wiki/Linux_kernel_interfaces>Linux system call interface</a>.
It provides an additional layer of isolation between running applications and the host operating system.</p><p>gVisor includes an Open Container Initiative (OCI) runtime called <code>runsc</code> that makes it easy to work with existing container tooling.
The <code>runsc</code> runtime integrates with Docker and Kubernetes, making it simple to run sandboxed containers.</p><p>gVisor intercepts application system calls and acts as the guest kernel, without the need for translation through virtualized hardware.
gVisor may be thought of as either a merged guest kernel and VMM, or as seccomp on steroids.
This architecture allows it to provide a flexible resource footprint (i.e. one based on threads and memory mappings, not fixed guest physical resources) while also lowering the fixed costs of virtualization.
However, this comes at the price of reduced application compatibility and higher per-system call overhead.</p><h3 id=kata-containers>Kata containers</h3><p><a href=https://katacontainers.io/>Kata containers</a> is an open-source project that aims to build a container runtime that works with lightweight virtual machines that feel like a container but provide the isolation of a virtual machine.</p><p>The Kata container runtime is CRI-compatible, which means that it is supported by Kubernetes.</p><p>You can <a href=https://github.com/kata-containers/documentation/blob/master/how-to/run-kata-with-k8s.md>run Kata containers in Kubernetes</a> by using either containerd or CRI-O.</p><h3 id=managing-instance-groups>Managing Instance Groups</h3><p>A Managing Instance Group(MIG) is a group of virtual machines as is treated as a single entity.</p><p>This feature is provided by cloud provider for creating and managing big fleets of virtual machines.</p><p>In the context of Kubernetes, you can have a MIG that is a set of nodes, which are labeled in the same way.
That way, you can have a Kubernetes cluster that runs on several MIGs, but different workloads get provisioned on different MIGs via <a href=#advanced-scheduling>Taints and Tolerations</a>.</p><h3 id=os-level-node-bootstrap>OS level Node bootstrap</h3><p>Bootstrapping a Kubernetes node involving configuring a machine in a way that it&rsquo;s able to function as a Kubernetes nodes.</p><p>That involves installing a container runtime, other Kubernetes related tools and configuring the cgroups and namespaces.</p><p>Tools like kubeadm help a lot with this process.
Full guide on bootstrapping a Kubernetes node you can find <a href=https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/_print/>here</a>.</p><h3 id=chaos-testingengineering>Chaos testing/engineering</h3><p><a href=https://en.wikipedia.org/wiki/Chaos_engineering>Chaos Engineering</a> is the discipline of experimenting on a system in order to build confidence in the system&rsquo;s capability to withstand turbulent conditions in production.</p><p><a href=https://www.pagerduty.com/resources/learn/what-is-chaos-testing/>Chaos Testing</a> is testing a system by destroying random parts of it.
In the case of software and micro-services architecture that could be disconnecting some of the nodes on which the software run and making sure that the other part of the system is still working or it has failed gracefully.</p><p>This methodology was made popular by Netflix, who runs a giant set of microservices.</p><p>If you are using Kubernetes, the simplest chaos test you can think of is deleting some pods or some nodes.
Kubernetes is designed for scenarios like these and it will automatically recreates the pods and try to reassign them on the remaining nodes.</p><p>There are tools like <a href=https://chaos-mesh.org/>Chaos Mesh</a> and <a href=https://chaostoolkit.org/drivers/kubernetes/>ChaosToolkit</a> that will allow you to execute more complex chaos testing scenarios in your Kubernetes cluster.</p><h3 id=cri>CRI</h3><p><a href=https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/>CRI</a> stands for Container Runtime Interface.</p><p>It defines an interface that is implemented by container runtime and called by Kubernetes to manage containers.</p><p>This allows for Kubernetes to support a variety of container runtimes and for those to be replaced without recompiling the Kubernetes code.
It also allows container runtime developers to develop the runtime independently, instead of Kubernetes in-tree and release new versions and bug-fixed ones decoupled from the Kubernetes release cycle.</p><p>Also, see <a href=https://kubernetes.io/docs/concepts/architecture/cri/>https://kubernetes.io/docs/concepts/architecture/cri/</a>.</p><h3 id=advanced-scheduling>Advanced scheduling</h3><p>The Kubernetes scheduler works pretty well in most cases - it makes sure that Pods are scheduled on Nodes that have enough resources, makes sure that ReplicaSets/StatefulSets are balanced across Nodes, etc.</p><p>However, sometimes we want to have even more fine-grained control.
For example, to schedule a Pod on a node that has some specific hardware, to dedicate Nodes to specific services, etc.</p><p>This is where the advanced scheduling concepts come in.
Most of these have been introduced in <a href=https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/>Kubernetes 1.6</a> and are already stable.</p><h4 id=node-affinityanti-affinity>Node affinity/anti-affinity</h4><p>Node affinity means that a Pod has <em>affinity</em> towards a Node.</p><p>On the Node side this is achieved via setting labels.</p><p>On the Pod side this is achieved via setting the <code>spec.affinity.nodeAffinity</code> property.</p><p>For example, this Pod spec will schedule the Pod only on Nodes that have the <code>kubernetes.is/os:linux</code> label:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>affinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#268bd2>nodeAffinity</span>:
</span></span><span style=display:flex><span>      <span style=color:#268bd2>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>        <span style=color:#268bd2>nodeSelectorTerms</span>:
</span></span><span style=display:flex><span>          - <span style=color:#268bd2>matchExpressions</span>:
</span></span><span style=display:flex><span>              - <span style=color:#268bd2>key</span>: kubernetes.io/os
</span></span><span style=display:flex><span>                <span style=color:#268bd2>operator</span>: In
</span></span><span style=display:flex><span>                <span style=color:#268bd2>values</span>:
</span></span><span style=display:flex><span>                  - linux
</span></span></code></pre></div><p><code>requiredDuringSchedulingIgnoredDuringExecution</code> means that Kubernetes will take this affinity into account when scheduling the Pod, but will ignore it once the Pod is scheduled, e.g. it will not reschedule the Pod if the Node gets this label removed.</p><p>The other option is <code>preferredDuringSchedulingIgnoredDuringExecution</code> which means that Kubernetes will <em>try to</em> find a Node that has this label to schedule the Pod on, but if it doesn&rsquo;t it will schedule the Pod on another Node.
The affinity will be ignored once the Pod is scheduled, e.g. Kubernetes will not reschedule the Pod if the Node gets this label removed.</p><h4 id=pod-affinityanti-affinity>Pod affinity/anti-affinity</h4><p>Pod affinity is similar to Node affinity, but it affects how Pods are scheduled relative to one-another.
For example, we might want to schedule two Pods together on the same Node for some reason.
We can do that with Pod affinity.</p><p>This is achieved via the <code>spec.affinity.podAffinity</code> property.</p><p>For example, this Pod spec will schedule the Pod on the same Node as the Pod that has the <code>service:S1</code> label:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>affinity</span>:
</span></span><span style=display:flex><span>    <span style=color:#268bd2>podAffinity</span>:
</span></span><span style=display:flex><span>      <span style=color:#268bd2>requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style=display:flex><span>        - <span style=color:#268bd2>labelSelector</span>:
</span></span><span style=display:flex><span>            <span style=color:#268bd2>matchExpressions</span>:
</span></span><span style=display:flex><span>              - <span style=color:#268bd2>key</span>: service
</span></span><span style=display:flex><span>                <span style=color:#268bd2>operator</span>: In
</span></span><span style=display:flex><span>                <span style=color:#268bd2>values</span>: [“S1”]
</span></span><span style=display:flex><span>          <span style=color:#268bd2>topologyKey</span>: failure-domain.beta.kubernetes.io/zone
</span></span></code></pre></div><p>Again, we can have <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>preferredDuringSchedulingIgnoredDuringExecution</code> which work the same way as with Node affinity.</p><h4 id=taints-and-tolerations>Taints and tolerations</h4><p>Taints and tolerations allow you to mark Nodes (taint them) and only schedule Pods that have a toleration for these taints on such a Node.</p><p>This way, you can mark your master node with a taint, so you can only schedule system components on them, or can mark nodes that run special hardware, so that only the workloads that need this hardware are schedules there.</p><p>You can add a Taint to a Node via <code>kubectl</code>:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl taint nodes node1 <span style=color:#268bd2>key</span><span style=color:#719e07>=</span>value:NoSchedule
</span></span></code></pre></div><p>and now on this Node we can only schedule Pods that have a Toleration for this Taint.</p><p>Tolerations are defined in the <code>spec.toleration</code> field of the Pod.</p><p>A toleration for this Taint will look like this:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>tolerations</span>:
</span></span><span style=display:flex><span>  - <span style=color:#268bd2>key</span>: <span style=color:#2aa198>&#34;key&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>operator</span>: <span style=color:#2aa198>&#34;Equal&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>value</span>: <span style=color:#2aa198>&#34;value&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#268bd2>effect</span>: <span style=color:#2aa198>&#34;NoSchedule&#34;</span>
</span></span></code></pre></div><h4 id=custom-schedulers>custom schedulers</h4><p>If none of this constructs gives the flexibility you need when scheduling your Pods you can write and deploy your own scheduler which will work alongside the default Kubernetes scheduler.
Actually, you can deploy as many schedulers as you want since Kubernetes supports running <a href=https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/>multiple schedulers</a>.</p><p>In order to utilize your custom scheduler, you need to set the <code>spec.schedulerName</code> field when creating a Pod.
This will tell the Kubernetes scheduler to not schedule the Pod.</p><p>For example:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>schedulerName</span>: my-custom-scheduler
</span></span></code></pre></div><p>tells Kubernetes that the <code>my-custom-scheduler</code> scheduler will schedule this Pod.</p><p>If such scheduler does not exist, this Pod will never get scheduled.</p><p>You can find a very simple implementation of a scheduler in Bash <a href=https://kubernetes.io/blog/2017/03/advanced-scheduling-in-kubernetes/>here</a>.</p><h3 id=metallb>MetalLB</h3><p><a href=https://metallb.universe.tf/>MetalLB</a> is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.</p><p>It aims to reduce the gap between bare-metal clusters and ones running in a cloud provider (<a href=https://aws.amazon.com/eks/>EKS</a>, <a href=https://azure.microsoft.com/en-us/services/kubernetes-service/>AKS</a>, <a href=https://cloud.google.com/kubernetes-engine>GKE</a>, etc.)</p><p>Bare-metal clusters that do not have the same LB-provisioning capabilities as the ones running on a cloud provider.
MetalLB aims to correct that and bring the same user experience when provisioning and configuring a Load Balancer.</p><h3 id=keepalived>Keepalived</h3><p><a href=https://www.keepalived.org/>Keepalived</a> is a routing software written in C.
The main goal of this project is to provide simple and robust facilities for load-balancing and high-availability to Linux system and Linux based infrastructures.
Loadbalancing framework relies on a well-known and widely used Linux Virtual Server (IPVS) kernel module providing Layer4 load-balancing.
Keepalived implements a set of checkers to dynamically and adaptively maintain and manage a load-balanced server pool according to their health.</p><p><a href=https://github.com/clastix/kubelived><strong>Kubelived</strong></a> is an open-source project that uses Keepalived for providing a cheap way to have highly-available Kubernetes control planes.</p><p>It works by deploying static pods that run Keepalived and assign the VIP(Virtual IP address) to one of the master nodes.
It then takes care that this VIP will always be assigned to a healthy node that will serve as load balancer for the whole cluster.</p><h2 id=summary>Summary</h2><p>This is all for part seven.</p><p>Just one more layer left until we crash the whole iceberg!</p><p>See you next week when I will publish the next and final article of these series (11 July 2022).</p><p>If you don’t want to miss it, you can follow me on <a href=https://twitter.com/a_sankov>Twitter</a> or <a href=https://www.linkedin.com/in/asankov/>LinkedIn</a>.</p><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/ aria-label="Previous - Demystifying the Kubernetes Iceberg: Part 6" class="btn btn-primary"><span class=mr-1>←</span>Previous</a></li><a class="btn btn-primary disabled">Next<span class=ml-1>→</span></a></li></ul></div></main></div></div><footer class="py-5 row d-print-none bg-nord0"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank rel=noopener href=mailto:asankov96@gmail.com aria-label="User mailing list"><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://twitter.com/a_sankov aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/asankov aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=LinkedIn aria-label=LinkedIn><a class=text-white target=_blank rel=noopener href=https://linkedin.com/in/asankov aria-label=LinkedIn><i class="fab fa-linkedin"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2022 Anton Sankov All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></small><p class=mt-2><a href=/about/>About Me</a></p></div></div></div></footer></div><script src=https://cdn.jsdelivr.net/npm/popper.js@1.16.1/dist/umd/popper.min.js integrity=sha384-9/reFTGAW83EW2RDu2S0VKaIzap3H66lZH81PoYlFhbGU+6BZp6G7niu735Sk7lN crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js integrity="sha512-UR25UO94eTnCVwjbXozyeVd6ZqpaAE9naiEUBK/A+QDbfSTQFhPGj5lOR6d8tsgbBk84Ggb5A3EkjsOgPRPcKA==" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script>
<script src=/js/main.min.1f7a9a77f4986d1b3c8c9d4e576bc7675ba7359139a0f97729bd28e9e2e1fd49.js integrity="sha256-H3qad/SYbRs8jJ1OV2vHZ1unNZE5oPl3Kb0o6eLh/Uk=" crossorigin=anonymous></script></body></html>