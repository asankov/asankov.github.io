<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.110.0"><meta name=robots content="index, follow"><link rel="shortcut icon" href=/favicons/favicon.ico><link rel=apple-touch-icon href=/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/favicons/android-192x192.png sizes=192x192><title>Demystifying the Kubernetes Iceberg: Part 5 | Anton Sankov's Blog</title><meta name=description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta property="og:title" content="Demystifying the Kubernetes Iceberg: Part 5"><meta property="og:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta property="og:type" content="article"><meta property="og:url" content="https://asankov.dev/blog/2022/06/12/demystifying-the-kubernetes-iceberg-part-5/"><meta property="article:section" content="blog"><meta property="article:published_time" content="2022-06-12T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-27T10:56:56+03:00"><meta property="og:site_name" content="Anton Sankov's Blog"><meta itemprop=name content="Demystifying the Kubernetes Iceberg: Part 5"><meta itemprop=description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><meta itemprop=datePublished content="2022-06-12T00:00:00+00:00"><meta itemprop=dateModified content="2022-06-27T10:56:56+03:00"><meta itemprop=wordCount content="2900"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Demystifying the Kubernetes Iceberg: Part 5"><meta name=twitter:description content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &#34;Kubernetes Iceberg&#34; diagram by Flant.
"><link rel=preload href=/scss/main.min.129dac5f6fd97995f99a6c3e77e483b8cfb38e14d03c450f75e68c569c07319d.css as=style><link href=/scss/main.min.129dac5f6fd97995f99a6c3e77e483b8cfb38e14d03c450f75e68c569c07319d.css rel=stylesheet integrity><script src=https://code.jquery.com/jquery-3.6.0.min.js integrity=sha384-vtXRMe3mGCbOeY7l30aIg8H9p3GdeSe4IFlP6G8JMa7o7lXvnz3GFKzPxzJdPfGK crossorigin=anonymous></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-7Y9WXX1393"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-7Y9WXX1393")}</script></head><body class="td-page td-blog"><header><nav class="js-navbar-scroll navbar navbar-expand navbar-dark flex-column flex-md-row td-navbar"><a class=navbar-brand href=/><span class="navbar-brand__logo navbar-logo"></span><span class=navbar-brand__name>Anton Sankov's Blog</span></a><div class="td-navbar-nav-scroll ml-md-auto" id=main_navbar><ul class="navbar-nav mt-2 mt-lg-0"><li class="nav-item mr-4 mb-2 mb-lg-0"><a class="nav-link active" href=/blog/><span class=active>Blog</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/about/><span>About Me</span></a></li><li class="nav-item mr-4 mb-2 mb-lg-0"><a class=nav-link href=/cv/><span>CV</span></a></li></ul></div><div class="navbar-nav d-none d-lg-block"></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><aside class="col-12 col-md-3 col-xl-2 td-sidebar d-print-none"><div id=td-sidebar-menu class=td-sidebar__inner><form class="td-sidebar__search d-flex align-items-center"><button class="btn btn-link td-sidebar__toggle d-md-none p-0 ml-3 fas fa-bars" type=button data-toggle=collapse data-target=#td-section-nav aria-controls=td-section-nav aria-expanded=false aria-label="Toggle section navigation"></button></form><nav class="collapse td-sidebar-nav" id=td-section-nav><ul class="td-sidebar-nav__section pr-md-3 ul-0"><li class="td-sidebar-nav__section-title td-sidebar-nav__section with-child active-path" id=m-blog-li><a href=/blog/ title="Anton Sankov's Blog" class="align-left pl-0 td-sidebar-link td-sidebar-link__section tree-root" id=m-blog><span>Blog</span></a><ul class=ul-1><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog2023011390daysofdevops-continuous-build-integration-and-testing-li><a href=/blog/2023/01/13/90daysofdevops-continuous-build-integration-and-testing/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog2023011390daysofdevops-continuous-build-integration-and-testing><span>90DaysOfDevOps - Continuous Build, Integration and Testing</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20221222implementing-the-java-stream-api-with-go-generics-part-1-li><a href=/blog/2022/12/22/implementing-the-java-stream-api-with-go-generics-part-1/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20221222implementing-the-java-stream-api-with-go-generics-part-1><span>Implementing the Java Stream API with Go Generics: Part 1</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220711demystifying-the-kubernetes-iceberg-part-8-li><a href=/blog/2022/07/11/demystifying-the-kubernetes-iceberg-part-8/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220711demystifying-the-kubernetes-iceberg-part-8><span>Demystifying the Kubernetes Iceberg: Part 8</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220704demystifying-the-kubernetes-iceberg-part-7-li><a href=/blog/2022/07/04/demystifying-the-kubernetes-iceberg-part-7/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220704demystifying-the-kubernetes-iceberg-part-7><span>Demystifying the Kubernetes Iceberg: Part 7</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220627demystifying-the-kubernetes-iceberg-part-6-li><a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220627demystifying-the-kubernetes-iceberg-part-6><span>Demystifying the Kubernetes Iceberg: Part 6</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child active-path" id=m-blog20220612demystifying-the-kubernetes-iceberg-part-5-li><a href=/blog/2022/06/12/demystifying-the-kubernetes-iceberg-part-5/ class="align-left pl-0 active td-sidebar-link td-sidebar-link__page" id=m-blog20220612demystifying-the-kubernetes-iceberg-part-5><span class=td-sidebar-nav-active-item>Demystifying the Kubernetes Iceberg: Part 5</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220605demystifying-the-kubernetes-iceberg-part-4-li><a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220605demystifying-the-kubernetes-iceberg-part-4><span>Demystifying the Kubernetes Iceberg: Part 4</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220529demystifying-the-kubernetes-iceberg-part-3-li><a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220529demystifying-the-kubernetes-iceberg-part-3><span>Demystifying the Kubernetes Iceberg: Part 3</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220522demystifying-the-kubernetes-iceberg-part-2-li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-2/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220522demystifying-the-kubernetes-iceberg-part-2><span>Demystifying the Kubernetes Iceberg: Part 2</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220515demystifying-the-kubernetes-iceberg-part-1-li><a href=/blog/2022/05/15/demystifying-the-kubernetes-iceberg-part-1/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220515demystifying-the-kubernetes-iceberg-part-1><span>Demystifying the Kubernetes Iceberg: Part 1</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220421securing-kubernetes-with-open-policy-agent-li><a href=/blog/2022/04/21/securing-kubernetes-with-open-policy-agent/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220421securing-kubernetes-with-open-policy-agent><span>Securing Kubernetes with Open Policy Agent</span></a></li><li class="td-sidebar-nav__section-title td-sidebar-nav__section without-child" id=m-blog20220129different-ways-to-initialize-go-structs-li><a href=/blog/2022/01/29/different-ways-to-initialize-go-structs/ class="align-left pl-0 td-sidebar-link td-sidebar-link__page" id=m-blog20220129different-ways-to-initialize-go-structs><span>Different Ways to Initialize Go structs</span></a></li></ul></li></ul></nav></div></aside><aside class="d-none d-xl-block col-xl-2 td-sidebar-toc d-print-none"><div class="td-page-meta ml-2 pb-1 pt-2 mb-0"></div><div class=td-toc><nav id=TableOfContents><ul><li><a href=#tier-5-part-2>Tier 5, Part 2</a><ul><li><a href=#multitenancy>Multitenancy</a></li><li><a href=#cert-manager>Cert-manager</a></li><li><a href=#certificate-renewal>Certificate renewal</a></li><li><a href=#cluster-autoscaler>cluster-autoscaler</a></li><li><a href=#egress-gateway>Egress gateway</a></li><li><a href=#descheduler>descheduler</a></li><li><a href=#custom-resources-validation-and-conversion>Custom Resources validation and conversion</a></li><li><a href=#etcd-cluster-management>etcd cluster management</a></li><li><a href=#kubernetes-upgrade>Kubernetes Upgrade</a></li><li><a href=#iac-for-grafana-dashboards-data-sources>IaC for Grafana (dashboards, data sources)</a></li><li><a href=#advanced-control-plane-configuration>Advanced control plane configuration</a></li><li><a href=#customizable-monitoring-for-all-kubernetes-objects>Customizable monitoring for all Kubernetes objects</a></li><li><a href=#long-term-prometheus>Long-term Prometheus</a></li><li><a href=#prometheus-query-caching>Prometheus Query Caching</a></li><li><a href=#ingress-monitoring>Ingress Monitoring</a></li><li><a href=#ingress-autoscaling>Ingress autoscaling</a></li><li><a href=#resource-sharing>Resource sharing</a></li><li><a href=#dynamic-storageclass-provisioning>Dynamic StorageClass provisioning</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div><div class="taxonomy taxonomy-terms-cloud taxo-categories"><h5 class=taxonomy-title>Categories</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://asankov.dev/categories/devops/ data-taxonomy-term=devops><span class=taxonomy-label>DevOps</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/gatekeeper/ data-taxonomy-term=gatekeeper><span class=taxonomy-label>Gatekeeper</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/go/ data-taxonomy-term=go><span class=taxonomy-label>Go</span><span class=taxonomy-count>2</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/kubernetes/ data-taxonomy-term=kubernetes><span class=taxonomy-label>Kubernetes</span><span class=taxonomy-count>9</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/open-policy-agent/ data-taxonomy-term=open-policy-agent><span class=taxonomy-label>Open Policy Agent</span><span class=taxonomy-count>1</span></a></li><li><a class=taxonomy-term href=https://asankov.dev/categories/security/ data-taxonomy-term=security><span class=taxonomy-label>Security</span><span class=taxonomy-count>2</span></a></li></ul></div></aside><main class="col-12 col-md-9 col-xl-8 pl-md-5 pr-md-4" role=main><div class=td-content><h1>Demystifying the Kubernetes Iceberg: Part 5</h1><div class=lead>Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the &ldquo;Kubernetes Iceberg&rdquo; diagram by Flant.</div><div class="td-byline mb-4"><time datetime=2022-06-12 class=text-muted>Sunday, June 12, 2022</time></div><header class=article-meta><div class="taxonomy taxonomy-terms-article taxo-categories"><h5 class=taxonomy-title>Categories:</h5><ul class=taxonomy-terms><li><a class=taxonomy-term href=https://asankov.dev/categories/kubernetes/ data-taxonomy-term=kubernetes><span class=taxonomy-label>Kubernetes</span></a></li></ul></div><p class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>&nbsp; 14 minute read &nbsp;</p></header><p>This is the fifth article of the &ldquo;Demystifying the Kubernetes Iceberg&rdquo; series.
My goal for this series is to explain all concepts mentioned in the “Kubernetes Iceberg” diagram by <a href=https://flant.com/>Flant</a>.</p><p>This is the iceberg:</p><p><img src=/images/kubernetes-iceberg.png alt="The Kubernetes Iceberg meme"></p><p>In this article, we will continue with Tier 5 of the iceberg.
We started with the <a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/>first part of Tier 5 last week</a>, and today we will pick up where we left off.</p><p>You can find the others articles here:</p><ul><li><a href=/blog/2022/05/15/demystifying-the-kubernetes-iceberg-part-1/>Part 1</a></li><li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-2/>Part 2</a></li><li><a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-3/>Part 3</a></li><li><a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/>Part 4</a></li></ul><p>I will publish one article each week until I complete the whole iceberg.</p><h2 id=tier-5-part-2>Tier 5, Part 2</h2><h3 id=multitenancy>Multitenancy</h3><p>Multitenancy in Kubernetes is not an easy problem to solve.
Part of the reason is that Kubernetes came out from Google, and multitenancy is usually not a necessity in the realm of a single company.</p><p>However, people have been trying to do multi-tenant Kubernetes for a while now.
There are some solutions and blueprints to how it could be done.
I would try to list the main ways to do that, with their pros and cons.</p><p>In these examples, we will assume that we are still talking about a single company, and the tenants are different teams inside that company.</p><h4 id=multitenancy-with-namespaces>Multitenancy with namespaces</h4><p>One way to do multitenancy in Kubernetes is via namespaces.</p><p>Each team(tenant) gets its own namespace.
Using <a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/#rbac>RBAC</a> we can assign permissions to the team members for only the namespace of their team.</p><p><strong>Pros:</strong></p><ul><li>Easy to do.
A new team comes - create a new namespace.</li><li>Does not require any additional software for maintenance.</li><li>Easy for a centralized team to enforce some sets of rules and standards for all teams using this cluster.</li></ul><p><strong>Cons:</strong></p><ul><li>Isolation is not ideal.<ul><li>User access is regulated via RBAC, but workloads (Pods) can still see other and talk to each other even when in separate namespaces.</li><li>Workloads will also run on a shared infrastructure, so a problem like a memory leak or container escape in one workload can easily affect other tenants’ workloads.</li><li>Some cluster-wide resources (like operators) can conflict with each other because there are many teams but just one cluster, and there can be only one such resource in a cluster.</li></ul></li><li>Requires additional management<ul><li>Since provisioning multiple clusters by hand will become pretty tedious quickly, this solution will almost always require an additional management layer on top of all your Kubernetes clusters.
This could be any of the cloud providers (AWS EKS, Azure AKS, GCP) or some self-hosted solution like VMware Tanzu TKG or Rancher.</li></ul></li></ul><h4 id=multitenancy-via-different-clusters>Multitenancy via different clusters</h4><p>Another way to do multitenancy in Kubernetes is to… not do multitenancy in Kubernetes.
Just provide a different cluster for every tenant.</p><p>Pros:</p><ul><li>Good isolation.
A problem in one tenant&rsquo;s workload will not affect other tenants.</li><li>Good autonomy.
Each team is the owner of their cluster, and they can do whatever they like.</li></ul><p>Cons:</p><ul><li>Expensive.
Each cluster has some resource overhead for the master nodes and other system components.
With this approach, this cost is multiplied by the number of teams.</li><li>It is harder for a centralized authority to enforce the same set of rules to every cluster in the organization because there will be many clusters.</li></ul><h4 id=multitenancy-via-virtual-clusters>Multitenancy via virtual clusters</h4><p>This approach aims to get the best of both worlds.
It involves deploying an additional layer that abstracts your namespaces and makes them look like separate clusters from the outside.</p><p>An existing solution is the <a href=https://github.com/loft-sh/vcluster>vcluster</a> project by <a href=https://loft.sh/>loft.sh</a>.</p><p>In short, how it works is that it gives access to a user to a cluster, which is just a namespace in the main cluster.
The user can deploy workloads there, but they will never be scheduled because they will actually be scheduled in the main namespace in the main cluster.</p><p>That way, it gives the tenants autonomy while still providing an easy way for cluster admins to enforce centralized rules and policies.</p><h3 id=cert-manager>Cert-manager</h3><p><a href=https://cert-manager.io/>cert-manager</a> is a X.509 certificate controller for Kubernetes.</p><p>It can be configured to obtain certificates from public Issuers (such as <a href=https://letsencrypt.org/>Let&rsquo;s Encrypt</a>) or private ones.
It is also responsible for keeping the certificates up-to-date, so it will attempt to renew any expiring ones.</p><p>Nowadays, using TLS for public connections is mandatory, but it is also recommended even for private service-to-service communication.
<code>cert-manager</code> is a valuable project that can help you a lot in being more secure in that aspect.</p><h3 id=certificate-renewal>Certificate renewal</h3><p>Certificate renewal is the process of renewing your SSL certificates.
This is necessary because each SSL certificate has an expiration date, after which it is not valid.</p><p>In the past, certificate issuers used to issue certificates with huge validity periods (5 years, for example).
This is now considered a bad practice, and browsers will usually reject certificates with validity bigger than 1 or 2 years.</p><p>Let&rsquo;s Encrypt issues certificates valid only for three months.
This is to encourage the use of automation when renewing certificates.</p><p>If you are using <code>cert-manager</code>, it can automatically handle the certificate renewal process.</p><h3 id=cluster-autoscaler>cluster-autoscaler</h3><p><a href=https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler>Kubernetes cluster-autoscaler</a> is a tool that automatically adjusts the size of the cluster (number of nodes) based on several factors, such as node utilization and failing pods.</p><p>It makes sure that there is a place for all Pods to run while at the same time it is not using more nodes than it could be.</p><p>There is an available implementation for most major cloud providers such as <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md>AWS</a>, <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md>Azure</a> or an <a href=https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/externalgrpc/README.md>external out-of-tree one</a>.</p><h3 id=egress-gateway>Egress gateway</h3><p>Egress gateway is a gateway for the outgoing traffic.</p><p>An egress gateway allows you to limit the outgoing traffic of your workloads.
This could be a useful security feature for preventing an attacker from making malicious network connections to the outside world.</p><p>This is not a native Kubernetes concept, but it is implemented by some Kubernetes network implementations (CNI), such as <a href="https://projectcalico.docs.tigera.io/about/about-kubernetes-egress#:~:text=enterprise%20deployment%20scenarios.-,Egress%20gateways,-Another%20approach%20to">Calico</a> and <a href=https://docs.cilium.io/en/stable/gettingstarted/egress-gateway/>Cilium</a> or by service meshes like <a href=https://istio.io/latest/docs/tasks/traffic-management/egress/egress-gateway/>Istio</a>.</p><h3 id=descheduler>descheduler</h3><p>The <a href=https://github.com/kubernetes-sigs/descheduler>descheduler</a> is a Kubernetes component that is responsible for descheduling workloads.</p><p>That can happen for a variety of reasons:</p><ul><li>Some nodes are under or overutilized.</li><li>Taints or labels are added to or removed from nodes, and pod/node affinity requirements are not satisfied anymore.</li><li>Some nodes failed, and their pods moved to other nodes.</li><li>New nodes are added to clusters.</li></ul><p>After the descheduler has descheduled the pods, they are back to the scheduler, and it is his responsibility to reschedule them again.</p><h3 id=custom-resources-validation-and-conversion>Custom Resources validation and conversion</h3><p><code>CustomResourceDefinitions</code> are a way to define custom resources and thus extend the Kubernetes API.</p><p>Each Custom Resource Definitions defines its fields via an OpenAPI spec.
For example, a <code>User</code> CRD can have the following API spec:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>schema</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>openAPIV3Schema</span>:
</span></span><span style=display:flex><span>    <span style=color:#268bd2>type</span>: object
</span></span><span style=display:flex><span>    <span style=color:#268bd2>properties</span>:
</span></span><span style=display:flex><span>      <span style=color:#268bd2>spec</span>:
</span></span><span style=display:flex><span>        <span style=color:#268bd2>type</span>: object
</span></span><span style=display:flex><span>        <span style=color:#268bd2>properties</span>:
</span></span><span style=display:flex><span>          <span style=color:#268bd2>id</span>:
</span></span><span style=display:flex><span>            <span style=color:#268bd2>type</span>: string
</span></span><span style=display:flex><span>          <span style=color:#268bd2>name</span>:
</span></span><span style=display:flex><span>            <span style=color:#268bd2>type</span>: string
</span></span></code></pre></div><p>We see that the <code>User</code> has two properties - <code>id</code> and <code>name</code>, both of type <code>string</code>.</p><p>The Open API spec of the resource can be extended to specify additional validation for the fields.
For example, you could define required properties via the <code>required</code> parameter or specify a regex validator for the string values via the <code>pattern</code> parameter:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>id</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>type</span>: string
</span></span><span style=display:flex><span>  <span style=color:#268bd2>required</span>: <span style=color:#cb4b16>true</span>
</span></span></code></pre></div><p>The validation for these fields will be performed by the Kubernetes API automatically, and if a user tries to create an object that violates these rules, the request will fail.</p><h3 id=etcd-cluster-management>etcd cluster management</h3><p><code>etcd</code> is a distributed key-value store.
By default, Kubernetes uses etcd as the place where all data is persisted.
For example, when we create a Pod resource, that gets persisted into etcd.</p><p>Since <code>etcd</code> is by-design, a distributed data store, most production environments run multiple etcd instances.
This way, even if one of them dies, our data will be safe in the other ones.</p><p>In order to work together, the <code>etcd</code> instance need to be aware of one another and be able to communicate with each other.
This requires some amount of configuration when starting the cluster.</p><p>Most of the important things to know when configuring an etcd cluster, like service discovery, DNS and TLS configuration, etc., are described <a href=https://etcd.io/docs/v3.4/op-guide/clustering/>here</a>.</p><p>Finally, a good security measure for <code>etcd</code> is to run it on dedicated master nodes, which are not publicly available, and are configured via Network Policies to be only accessible to the API server.
This is because we should never interact directly with etcd, only through the API server.
Also, if an attacker got access to etcd, they could damage our cluster (for example, by deleting resources.)</p><h3 id=kubernetes-upgrade>Kubernetes Upgrade</h3><p>Upgrading your Kubernetes cluster is an important thing in the cluster lifecycle.
New Minor Kubernetes versions (v1.XX.0) are released every four months, with patch versions (v1.24.XX) released more often to address bugs and security vulnerabilities.</p><p>New Kubernetes versions provide new features, and old Kubernetes versions eventually reach End-Of-Life, and support for them is dropped.
You should always be running a supported Kubernetes version.</p><p>For people running managed Kubernetes (e.g., EKS, AKS, GKE, Rancher, etc.), upgrading your Kubernetes cluster can be as simple as switching a value in a drop-down menu in the cloud-provider UI.</p><p>That is not the case, if you are managing your Kubernetes cluster on your own.
If so, you would need to upgrade your cluster manually.
By &ldquo;manually,&rdquo; I mean there is a helpful tool that manages most of the heavy lifting, but you will still be responsible for using it properly.</p><p>That tool is called <a href=https://github.com/kubernetes/kubeadm><code>kubeadm</code></a> and helps not only for upgrading a cluster but also for setting it up.</p><p>The process of upgrading a cluster is described in detail <a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/>here</a>, but the basic steps to follow are these:</p><ul><li>BACKUP your data!</li><li>upgrade your master node one by one</li><li>upgrade your CNI</li><li>upgrade your worker nodes one by one</li></ul><h3 id=iac-for-grafana-dashboards-data-sources>IaC for Grafana (dashboards, data sources)</h3><p>IaC stands for Infrastructure as Code.
This paradigm involves describing your infrastructure in text files (code) and having a tool that will provision/destroy resources based on the contents of this file and the changes to it.
Such tools are <a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/#terraform>Terraform</a>, Pulumi and others.</p><p>Most IaC tools like Terraform can provision absolutely everything given that there is a provider for it.
A provider is an implementation that provisions infrastructure based on your code.</p><p>There is such provider for <a href=https://registry.terraform.io/providers/grafana/grafana/latest/docs>Grafana</a>.
Using it, you can describe your dashboards and data sources as code and get these automatically created for you by your IaC tool.</p><p>For example, this is a Terraform code snippet that will create a Grafana dashboard based on the <code>grafana-dashboard.json</code> JSON file:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-terraform data-lang=terraform><span style=display:flex><span><span style=color:#268bd2>resource</span> <span style=color:#2aa198>&#34;grafana_dashboard&#34;</span> <span style=color:#2aa198>&#34;metrics&#34;</span> {
</span></span><span style=display:flex><span>  config_json = <span style=color:#b58900>file</span>(<span style=color:#2aa198>&#34;grafana-dashboard.json&#34;</span>)
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>It is also possible to manually configure your dashboards by hand and then export the Terraform code for them.
This way, you get the best of both worlds - manual configuration maintained by your IaC tool.</p><h3 id=advanced-control-plane-configuration>Advanced control plane configuration</h3><p>The Kubernetes control plane and its components (<code>api-server</code>, <code>controller-manager</code>, <code>scheduler</code>, and <code>etcd</code>) support some customization depending on the user&rsquo;s needs.</p><p>This customization is different for each component, but if I have to point out some of the most important things for each one:</p><h4 id=api-serverhttpskubernetesiodocsreferencecommand-line-tools-referencekube-apiserver><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/>API server</a></h4><ul><li>audit log configuration - max size, the place to be stored, retention, etc.</li><li>TLS certificates</li><li>configuration of the leader-elect algorithm</li><li>enabling and disabling optional features</li><li>metrics</li></ul><h4 id=controller-managerhttpskubernetesiodocsreferencecommand-line-tools-referencekube-controller-manager><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/>Controller manager</a></h4><ul><li>TLS certificates</li><li>configuration of the leader-elect algorithm</li><li>enabling and disabling optional features</li><li>metrics</li></ul><h4 id=schedulerhttpskubernetesiodocsreferencecommand-line-tools-referencekube-scheduler><a href=https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/>Scheduler</a></h4><ul><li>TLS certificates</li><li>configuration of the leader-elect algorithm</li><li>enabling and disabling optional features</li><li>metrics</li></ul><h4 id=etcdhttpsetcdiodocs><a href=https://etcd.io/docs/>etcd</a></h4><ul><li>TLS certificates</li><li>metrics</li><li>clustering configuration</li></ul><h3 id=customizable-monitoring-for-all-kubernetes-objects>Customizable monitoring for all Kubernetes objects</h3><p>Fully monitoring all your Kubernetes objects is vital in order to have complete visibility of the state of your cluster and be able to act accordingly when needed.</p><p>Kubernetes provides you with two ways to monitor your resources.
The first one is the <strong>resource metrics pipeline</strong>, which gives you a limited set of metrics related to cluster components.
They are saved in a short-term in-memory metrics server and exposed via the <code>metrics.k8s.io</code> API or via the <code>kubectl top</code> utility.</p><p>The second one is the <strong>full metrics pipeline</strong>, which is more sophisticated and gives you more metrics to work with.
These are exposed by implementing either of the <code>custom.metrics.k8s.io</code> or <code>external.metrics.k8s.io</code> APIs.</p><p>A CNCF-supported implementation of these APIs is <a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/#prometheus>Prometheus</a>.</p><h3 id=long-term-prometheus>Long-term Prometheus</h3><p><a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/#prometheus>Prometheus</a> is an open-source monitoring and alerting toolkit.</p><p>It is used for metrics collection and aggregation.</p><p>It can also be integrated with <a href=https://prometheus.io/docs/prometheus/latest/storage/#local-storage>local</a> or <a href=https://prometheus.io/docs/prometheus/latest/storage/#remote-storage-integrations>remote</a> file storage to achieve bigger data retention.</p><p>The file storage options are configured via the <a href=https://prometheus.io/docs/prometheus/latest/storage/#operational-aspects><code>--storage.XXX</code> command line arguments</a>.</p><p>When using long-term storage, it is advisable to lower the number of time series scrapes in order to save space (and also because you probably would not care about second-by-second of your 6-month-old metrics).</p><h3 id=prometheus-query-caching>Prometheus Query Caching</h3><p>Prometheus queries are written in <a href=/blog/2022/05/29/demystifying-the-kubernetes-iceberg-part-3/#promql>PromQL</a>.</p><p>PromQL is quite powerful and can do many things like summing, averages, aggregation, etc.
These are heavy operations that, if executed on a large dataset, can take a significant amount of CPU and memory to complete.</p><p>That is why Prometheus front-ends like Grafana (and Prometheus itself) support query caching, e.g., saving the results of a given query for some time, and if another user runs the same query on the same data, it will return the cached results.
This speeds up the process of fetching the data and avoids unnecessary computations.</p><p>Of course, this is a trade-off because in a real-time system like Prometheus, the data changes by the second, so we cannot set too big a cache TTL without risking showing our users outdated data.</p><h3 id=ingress-monitoring>Ingress Monitoring</h3><p>The <a href=/blog/2022/05/15/demystifying-the-kubernetes-iceberg-part-1/#ingress>ingress</a> manages the external traffic coming into the cluster.</p><p>It is an integral part of our system - if the ingress is down or it cannot scale, our whole system will be blocked because all requests go through the ingress before getting into it.</p><p>That is why it&rsquo;s important to have monitoring in place so that we know at any given time what traffic flows into our system, what the latencies are, are there any problems, etc.</p><p>Since the <code>Ingress</code> resource is an abstract one, all functionality is implemented by the controllers.
So is the monitoring.
Different ingress controller implementations provide different monitoring constructs, but they all achieve the same results.</p><p>For example, if you are running the <a href=https://docs.nginx.com/nginx-ingress-controller>NGINX Ingress controller</a>, you can enable metrics via the <code>-enable-prometheus-metrics</code> flag, and then you can consume the metrics listed <a href=https://docs.nginx.com/nginx-ingress-controller/logging-and-monitoring/prometheus/>here</a>.</p><h3 id=ingress-autoscaling>Ingress autoscaling</h3><p>After enabling ingress metrics, you can take it to the next level and autoscale your ingress based on these metrics.
For example, spin up new instances once you have a traffic peak and the network latency starts growing.</p><p>It&rsquo;s vital that your ingress is scaled appropriately because this is the entry point of the traffic to your application.
If not scaled correctly, the ingress could be a bottleneck which can slow your application and cause frustration to your users.</p><p>Just like the monitoring, autoscaling is also provided by the controller implementations.</p><p>For example, if you are running the <a href=https://docs.nginx.com/nginx-ingress-controller>NGINX Ingress controller</a>, you can configure autoscaling via the KEDA autoscaler that consumes the nginx metrics.</p><p>This object configures KEDA to autoscale the NGINX Pods if the average connections for a minute are more than 100.</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>apiVersion</span>: keda.sh/v1alpha1
</span></span><span style=display:flex><span><span style=color:#268bd2>kind</span>: ScaledObject
</span></span><span style=display:flex><span><span style=color:#268bd2>metadata</span>:
</span></span><span style=display:flex><span> <span style=color:#268bd2>name</span>: nginx-scale
</span></span><span style=display:flex><span><span style=color:#268bd2>spec</span>:
</span></span><span style=display:flex><span> <span style=color:#268bd2>scaleTargetRef</span>:
</span></span><span style=display:flex><span>   <span style=color:#268bd2>kind</span>: Deployment
</span></span><span style=display:flex><span>   <span style=color:#268bd2>name</span>: main-nginx-ingress
</span></span><span style=display:flex><span><span style=color:#268bd2>minReplicaCount</span>: <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>maxReplicaCount</span>: <span style=color:#2aa198>20</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>cooldownPeriod</span>: <span style=color:#2aa198>30</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>pollingInterval</span>: <span style=color:#2aa198>1</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>triggers</span>:
</span></span><span style=display:flex><span>- <span style=color:#268bd2>type</span>: prometheus
</span></span><span style=display:flex><span>   <span style=color:#268bd2>metadata</span>:
</span></span><span style=display:flex><span>     <span style=color:#268bd2>serverAddress</span>: http://prometheus-server
</span></span><span style=display:flex><span>     <span style=color:#268bd2>metricName</span>: nginx_connections_active_keda
</span></span><span style=display:flex><span>     <span style=color:#268bd2>query</span>: |<span style=color:#2aa198>
</span></span></span><span style=display:flex><span><span style=color:#2aa198>       </span>       sum(avg_over_time(nginx_ingress_nginx_connections_active{app=&#34;main-nginx-ingress&#34;}[1m]))
</span></span><span style=display:flex><span>     <span style=color:#268bd2>threshold</span>: <span style=color:#2aa198>&#34;100&#34;</span>
</span></span></code></pre></div><h3 id=resource-sharing>Resource sharing</h3><p>Kubernetes runs multiple workloads (Pods) on the same physical/virtual Node.
This means that these workloads will share the underlying host resources.</p><p>Kubernetes provides two constructs to control how these resources are shared - resource limits and resource requests.</p><p>We discussed resource limits in <a href=/blog/2022/05/22/demystifying-the-kubernetes-iceberg-part-2>Part 2</a> of this series, but now I will go into more detail about the limits and requests and the difference between the two.</p><p>The <em>requests</em> show how many minimum resources the workload will need.
The kubelet will use this information to find a proper Node to schedule the Pod (one with at least that many resources as the requests for the Pod).
A Pod can go under or over the requested resources.</p><p>The <em>limits</em> show how many maximum resources the workload will need.
Kubernetes will not allow a resource to use more than its limits.
This limit is enforced by the container runtime, and a resource will not be allowed to exceed it.</p><h3 id=dynamic-storageclass-provisioning>Dynamic StorageClass provisioning</h3><p>When you create a Volume in Kubernetes, you specify its <code>StorageClass</code>.
The <code>StorageClass</code> shows some properties of the volume, e.g., its quality-of-service levels, its backup policies, or other policies determined by the cluster administrator.</p><p>You are not limited to the number of <code>StorageClasses</code> you can use, and you can even define your own <code>StorageClasses</code> by creating a new Kubernetes resource of that type.</p><p>This is a sample <code>StorageClass</code>:</p><div class=highlight><pre tabindex=0 style=color:#93a1a1;background-color:#002b36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#268bd2>apiVersion</span>: storage.k8s.io/v1
</span></span><span style=display:flex><span><span style=color:#268bd2>kind</span>: StorageClass
</span></span><span style=display:flex><span><span style=color:#268bd2>provisioner</span>: kubernetes.io/aws-ebs
</span></span><span style=display:flex><span><span style=color:#268bd2>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>name</span>: standard
</span></span><span style=display:flex><span><span style=color:#268bd2>reclaimPolicy</span>: Retain
</span></span><span style=display:flex><span><span style=color:#268bd2>allowVolumeExpansion</span>: <span style=color:#cb4b16>true</span>
</span></span><span style=display:flex><span><span style=color:#268bd2>volumeBindingMode</span>: Immediate
</span></span><span style=display:flex><span><span style=color:#268bd2>parameters</span>:
</span></span><span style=display:flex><span>  <span style=color:#268bd2>type</span>: gp2
</span></span><span style=display:flex><span><span style=color:#268bd2>mountOptions</span>:
</span></span><span style=display:flex><span>  - debug
</span></span></code></pre></div><p>From its properties, we see that volumes from this class are provisioned by AWS EBS; it has the GP2 type, reclaim policy of <code>Retain</code>, etc.
The <code>parameters</code> depend on the provisioner, e.g., AWS EBS has one set of parameters that make sense for this provisioner, GCE PD has other parameters, and so on.</p><h2 id=summary>Summary</h2><p>This is all for part five.</p><p>In the last two article, we managed to demystify the biggest tier of the iceberg.
The next ones are smaller, but are getting more and more specific.
I don&rsquo;t know about you, but I can&rsquo;t wait to dive into them.</p><p>The series continues with <a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/>Part 6</a>.</p><p>If you don’t want to miss it, you can follow me on <a href=https://twitter.com/a_sankov>Twitter</a> or <a href=https://www.linkedin.com/in/asankov/>LinkedIn</a>.</p><ul class="list-unstyled d-flex justify-content-between align-items-center mb-0 pt-5"><li><a href=/blog/2022/06/05/demystifying-the-kubernetes-iceberg-part-4/ aria-label="Previous - Demystifying the Kubernetes Iceberg: Part 4" class="btn btn-primary"><span class=mr-1>←</span>Previous</a></li><li><a href=/blog/2022/06/27/demystifying-the-kubernetes-iceberg-part-6/ aria-label="Next - Demystifying the Kubernetes Iceberg: Part 6" class="btn btn-primary">Next<span class=ml-1>→</span></a></li></ul></div></main></div></div><footer class="bg-dark py-5 row d-print-none"><div class="container-fluid mx-sm-5"><div class=row><div class="col-6 col-sm-4 text-xs-center order-sm-2"><ul class="list-inline mb-0"><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title="User mailing list" aria-label="User mailing list"><a class=text-white target=_blank rel=noopener href=mailto:asankov96@gmail.com aria-label="User mailing list"><i class="fa fa-envelope"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=Twitter aria-label=Twitter><a class=text-white target=_blank rel=noopener href=https://twitter.com/a_sankov aria-label=Twitter><i class="fab fa-twitter"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=GitHub aria-label=GitHub><a class=text-white target=_blank rel=noopener href=https://github.com/asankov aria-label=GitHub><i class="fab fa-github"></i></a></li><li class="list-inline-item mx-2 h3" data-toggle=tooltip data-placement=top title=LinkedIn aria-label=LinkedIn><a class=text-white target=_blank rel=noopener href=https://linkedin.com/in/asankov aria-label=LinkedIn><i class="fab fa-linkedin"></i></a></li></ul></div><div class="col-6 col-sm-4 text-right text-xs-center order-sm-3"></div><div class="col-12 col-sm-4 text-center py-2 order-sm-2"><small class=text-white>&copy; 2023 Anton Sankov All Rights Reserved</small>
<small class=ml-1><a href=https://policies.google.com/privacy target=_blank rel=noopener>Privacy Policy</a></small><p class=mt-2><a href=/about/>About Me</a></p></div></div></div></footer></div><script src=/js/main.min.89601081e7acbb4f82b6e87a14261c545bc836fd9542b1cf596a332fa30bc1c2.js integrity="sha256-iWAQgeesu0+Ctuh6FCYcVFvINv2VQrHPWWozL6MLwcI=" crossorigin=anonymous></script>
<script src=/js/tabpane-persist.js></script></body></html>