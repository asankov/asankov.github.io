
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Demystifying the Kubernetes Iceberg: Part 2</title>
    <meta name="description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This article explains all the concepts listed in the "Kubernetes Iceberg" meme by Flant." />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-2" />
    <meta property="og:title" content="Demystifying the Kubernetes Iceberg: Part 2" />
    <meta property="og:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This article explains all the concepts listed in the "Kubernetes Iceberg" meme by Flant." />
    <meta property="og:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-2" />
    <meta property="twitter:title" content="Demystifying the Kubernetes Iceberg: Part 2" />
    <meta property="twitter:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This article explains all the concepts listed in the "Kubernetes Iceberg" meme by Flant." />
    <meta property="twitter:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <link rel="stylesheet" href="/index.css">
    <script type="module" crossorigin src="/assets/index-xxxxxxxx.js"></script> <!-- Placeholder for Vite's main JS bundle -->
</head>
<body>
    <div id="root"></div>
    <script>
        // Store pre-rendered content for client-side hydration
        window.__PRELOADED_STATE__ = {
            post: {
                slug: "demystifying-the-kubernetes-iceberg-2",
                title: "Demystifying the Kubernetes Iceberg: Part 2",
                description: "Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This article explains all the concepts listed in the "Kubernetes Iceberg" meme by Flant.",
                date: "2022-05-22T00:00:00.000Z",
                content: "<p>This is the second article of the &quot;Demystifying the Kubernetes Iceberg&quot; series.\nMy goal for this series is to explain all concepts mentioned in the “Kubernetes Iceberg” meme by <a href=\"https://flant.com/\">Flant</a>.</p>\n<p>You can find the first article <a href=\"/post/demystifying-the-kubernetes-iceberg-1/\">here</a>.\nI will publish one article each week until I complete the whole iceberg.</p>\n<p>And this is the iceberg itself:</p>\n<p><img src=\"/images/kubernetes-iceberg.png\" alt=\"The Kubernetes Iceberg meme\"></p>\n<p>In this article, I focus on Tier 3 of the Iceberg.\nLet’s go!</p>\n<h2>Tier 3</h2>\n<h3>CRD</h3>\n<p><code>CRD</code> or a <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/\"><code>Custom Resource Definition</code></a> is a way to extend the Kubernetes API by adding your own resource.</p>\n<p>You can define what fields it has, what types they are, whether any of them is required, their default values, etc.</p>\n<p>Kubernetes will register CRUD API endpoints for this resource (get, list, watch, delete, etc.) and you can start treating this resource as any of the Kubernetes build-in ones (Pods, Deployments, etc.).</p>\n<p>This includes:</p>\n<ul>\n<li>using <code>kubectl</code> to <code>get</code> or <code>describe</code> this resource</li>\n<li>creating instances of this resource via YAML files and <code>kubectl apply</code></li>\n<li>calling the REST APIs for this resource directly, e.g. <code>curl &lt;kube_url&gt;/api/&lt;version&gt;/&lt;crd_name&gt;</code></li>\n</ul>\n<p>If you want to have an additional logic around your resource, e.g. trigger some actions when someone creates an instance of our CRD, you can do that by implementing an operator (more on that in later posts).</p>\n<p>You can check out my <a href=\"https://www.youtube.com/watch?v=yim8NnYjODY\">presentation</a> from ISTA 2021 about how to use CRD to build a CRUD application backend.</p>\n<h3>PersistentVolumeClaim</h3>\n<p>A <code>PersistentVolumeClaim</code> is a request for storage by a user/resource.\nThe given storage is taken from a <a href=\"https://kubernetes.io/docs/concepts/storage/persistent-volumes/\"><code>PersistentVolume</code></a>, which is an abstract Kubernetes object that represents some storage.\nThere are different storage classes, that represent the different implementations of storage that can be used by Kubernetes.\nThe storage class is specified in the <code>PersistentVolume</code> spec.</p>\n<p>Once a <code>PersistentVolumeClaim</code> is created, Kubernetes will try to utilize the storage by provisioning the requested amount to the requester.</p>\n<p>The <code>PersistentVolumeClaim</code> also specified the access mode to the storage (e.g. <code>ReadWriteOnce</code>, <code>ReadOnlyMany</code>, or <code>ReadWriteMany</code>).</p>\n<h3>StatefulSet</h3>\n<p>A <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\"><code>StatefulSet</code></a> is a Kubernetes resource the represents a workload.\nIt is similar to a <a href=\"/post/demystifying-the-kubernetes-iceberg-1/#deployments\"><code>Deployment</code></a> - it manages a set of Pods based on a Pod spec.\nThe specific thing about a <code>StatefulSet</code> is that manages stateful applications (hence the name).</p>\n<p>It is useful for situations where you want:</p>\n<ul>\n<li>stable, unique network identifiers</li>\n<li>stable, persistent storage</li>\n<li>ordered, graceful deployment and scaling</li>\n<li>ordered, automatic rolling updates</li>\n</ul>\n<h3>Helm templating</h3>\n<p><a href=\"https://helm.sh/\">Helm</a> is a templating engine that allows you to write reusable Kubernetes templates.</p>\n<p>For example, if you have a Pod that you deploy to three environments and you want to set a different label for each environment, you don&#39;t need to copy-paste the Pod YAML three times and just change the value of the label.</p>\n<p>You can use Helm to extract the different fields into variables, which later get templated, and reuse all the parts that don&#39;t differ between environments.</p>\n<p>For example, this is a template for a Pod with a hardcoded <code>environment</code> label:</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-sleep\n  labels:\n    environment: production\nspec:\n  containers:\n    - name: busybox\n      image: busybox\n      args:\n        - sleep\n        - &quot;1000000&quot;\n</code></pre>\n<p>If we want to convert this to a Helm template all we need to do is:</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-sleep\n  labels:\n    environment: {{ .Values.environment }}\nspec:\n  containers:\n    - name: busybox\n      image: busybox\n      args:\n        - sleep\n        - &quot;1000000&quot;\n</code></pre>\n<p>Now the value of the <code>environment</code> label comes from the <code>.Values.environment</code> variable.</p>\n<p>This variable comes from a so-called values file.</p>\n<p>This is a YAML in which we specify the values of our variables.</p>\n<p>By using different values files we can output different resource specs with different values.</p>\n<p>For example, we can have one values files for each of our environments:</p>\n<pre><code class=\"language-yaml\"># dev.yaml\nenvironment: dev\n</code></pre>\n<pre><code class=\"language-yaml\"># staging.yaml\nenvironment: staging\n</code></pre>\n<pre><code class=\"language-yaml\"># production.yaml\nenvironment: production\n</code></pre>\n<p>Using the template shown above with the <code>dev.yaml</code> values file will output this spec:</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-sleep\n  labels:\n    environment: dev\nspec:\n  containers:\n    - name: busybox\n      image: busybox\n      args:\n        - sleep\n        - &quot;1000000&quot;\n</code></pre>\n<p>While using it with the <code>production.yaml</code> values file will output this one:</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox-sleep\n  labels:\n    environment: production\nspec:\n  containers:\n    - name: busybox\n      image: busybox\n      args:\n        - sleep\n        - &quot;1000000&quot;\n</code></pre>\n<h4>Package manager</h4>\n<p>Apart from a templating engine, Helm is also a package manager for Kubernetes.\nYou can use it to &quot;package&quot; your Helm charts (a Helm chart == a bunch of templates) and publish them into a repository.\nOnce you do that, everyone can install you chart into their cluster via the <code>helm install</code> command and use Helm to manage the lifecycle of that chart.</p>\n<h3>Dashboard</h3>\n<p>A dashboard is a type of graphical user interface which provides visual information about some metrics in a given system.</p>\n<p>A Kubernetes dashboard would show the number of Pods, Deployment, Services, Nodes, etc.</p>\n<p>There is an “official” dashboard provided by Kubernetes, which you can install in your cluster.\nIt is a web-based one, that shows you all the resources in your cluster and a lot of other useful information. It looks like this:</p>\n<p><img src=\"/images/kubernetes-dashboard.png\" alt=\"Screenshot from the Kubernetes dashboard application\">\n<a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\">Source of the image</a></p>\n<p>To install it, you need to apply some Kubernetes resource and expose them outside of the cluster (so that you can open the web page in your browser).</p>\n<p>More information about the dashboard, and how to install it you can find <a href=\"https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/\">here</a>.</p>\n<p>Apart from this “official” dashboard there are other open-source projects which you can use to get the same information.\nThey are called Kubernetes IDEs and they provide similar experience to the Kubernetes dashboard.</p>\n<p>Some of them are:</p>\n<h4><a href=\"https://k9scli.io/\">K9s</a></h4>\n<p>Open-source CLI-based Kubernetes IDE.\nConnects to the cluster(s) defined in your kubeconfig file.</p>\n<p>Good for users that prefer the terminal.\nSupports many “power-user” workflows.</p>\n<p><img src=\"/images/k9s.png\" alt=\"Screenshot from the K9s application\"></p>\n<h4><a href=\"https://k8slens.dev/\">Lens</a></h4>\n<p>Open-source Desktop application based on Electron.</p>\n<p>Build by the Lens team, now part of <a href=\"https://www.mirantis.com/\">Mirantis</a>.\nSupports adding multiple Kubernetes clusters and switching between them.</p>\n<p><img src=\"/images/lens.png\" alt=\"Screenshot from the Lens application\"></p>\n<h4><a href=\"https://octant.dev/\">Octant</a></h4>\n<p>Open-source Web-based dashboard, by <a href=\"https://www.vmware.com/\">VMware</a>.</p>\n<p>Can be run locally or deployed into a remote cluster and accessed via the browser.</p>\n<p><img src=\"/images/octant.png\" alt=\"Screenshot from the Octant application\"></p>\n<h3>HPA</h3>\n<p>HPA or <a href=\"https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/\"><code>HorizontalPodAutoscaler</code></a> is a Kube API server feature that allows you to scale workloads horizontally based on metrics.</p>\n<p>Horizontal scaling means that we are increasing the count of the running workloads (e.g. go from 3 Pods to 5).\nThis is different than vertical scaling, which means increasing the resources on a given workloads (e.g. increase the CPU and memory provided to a Pod).</p>\n<p>The <code>HorizontalPodAutoscaler</code> is created as a Kubernetes resource, part of the <code>autoscaling</code> API group.\nIt defines the workloads selector and the metrics based on which we’ll do the scaling.\nOnce on every interval (that is defined by the user, with a default value) the Kube API server will query the metrics of the Pods, and based on the conditions we defined will decide whether it needs to scale the workloads up (more pods) or down (less pods).</p>\n<p>There is always a min and max Pod count, e.g. never go below a certain number of Pods, and never go above a certain number of Pods.</p>\n<p>There are some default metrics that are defined by Kubernetes that can be used with <code>HorizontalPodAutoscaler</code> out of the box, but there is also a capability to use it with custom metrics, defined by you (more on that in the next article).</p>\n<h3>Log management</h3>\n<p>When running containers, they are usually configured to output their logs to <code>stdout</code> and <code>stderr</code> (at least, this is the best practice).\nIf the container dies (because of an issue or because we are upgrading the application), the logs will be lost (because the two output streams will go away with the container).</p>\n<p>This means that we need to plug in something else that will read these logs and store them permanently in a way not tied to the container lifecycle.</p>\n<p>Fortunately, Kubernetes has a solution for this.\nIt supports a lot of log drivers, which can be integrated with your application logging (without having custom logic in your app).\nThese drivers will read data from the container output streams and send the logs to a centralized system like <a href=\"https://logz.io/\">logz.io</a>, <a href=\"https://www.splunk.com/\">Splunk</a>, or <a href=\"https://grafana.com/oss/loki/\">Loki</a>.\nOnce there your logs are safe and they will not be lost, even if a container dies.\nAlso, a lot of these systems provide an easy way to aggregate logs from different services and search through the logs based on a keyword, or field-based search (if you have structured logging).</p>\n<h3>Init containers</h3>\n<p>Init containers are specialized containers that run before app containers in a Pod.</p>\n<p>They can be used to execute some actions in the Pod to prepare the Pod for the execution of the main container.</p>\n<p>You can have multiple init containers in a Pod. Kubernetes will run all of them before running the main container.</p>\n<p>All init containers need to exit successfully for Kubernetes to start the main container. If an init container fails, Kubernetes will restart it</p>\n<p>You can specify init containers via the <code>spec.initContainers</code> field in the Pod spec.</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-app-pod\n  labels:\n    app: my-app\nspec:\n  containers:\n    - name: my-app-container\n      image: busybox\n      command: [&quot;sh&quot;, &quot;-c&quot;, &quot;echo The app is running! &amp;&amp; sleep 3600&quot;]\n  initContainers:\n    - name: my-app-first-init-container\n      image: busybox\n      # print &quot;Hello from init container 1&quot; 5 times then exit successfully\n      command:\n        [\n          &quot;sh&quot;,\n          &quot;-c&quot;,\n          &quot;for i in {1..5}; do echo &#39;Hello from init container 1&#39;; done; exit 0&quot;,\n        ]\n    - name: my-app-second-init-container\n      image: busybox\n      # print &quot;Hello from init container 1&quot; 10 times then exit successfully\n      command:\n        [\n          &quot;sh&quot;,\n          &quot;-c&quot;,\n          &quot;for i in {1..10}; do echo &#39;Hello from init container 2&#39;; done; exit 0&quot;,\n        ]\n</code></pre>\n<h3>Affinity</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\">Affinity</a> is the capability to specify the preferred nodes for a workload, e.g. a workload has <strong>affinity</strong> towards a node.</p>\n<p>You can do that by setting labels on the Nodes, and then setting the affinity towards these labels in the Pod spec of the workloads.</p>\n<h3>Taints and Tolerations</h3>\n<p><a href=\"https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\">Taints and Tolerations</a> are the opposite of Affinity.\nWhilst the affinity defines the preferred Nodes for a Pod, Taints and Tolerations define the opposite - Nodes, where the Pods should NOT be scheduled on.</p>\n<p>Each Node can have multiple taints.\nIn order for a Pod to be able to be scheduled on a Node, a Pod must have Tolerations for all the Taints on that Node.</p>\n<h4>Effects</h4>\n<p>Taints/Tolerations can have different effects, which mean different things:</p>\n<ul>\n<li><code>NoSchedule</code> - if a Pod does not have a matching Toleration, Kubernetes will not schedule it on that Node. The Pod will keep running, if it was scheduled, before the Taints was added.</li>\n<li><code>PreferNoSchedule</code> - if a Pod does not have a matching Toleration, Kubernetes will <em>try to</em> not schedule it on that Node, but it will, if it has no other available Nodes. The Pod will keep running, if it was scheduled, before the Taints was added.</li>\n<li><code>NoExecute</code> - if a Pod does not have a matching Toleration, Kubernetes will not schedule it on that Node. If the Pod is already running on the Node, Kubernetes will evict it and re-schedule it on a new Node.</li>\n</ul>\n<h4>Use-cases</h4>\n<p>Some use-cases for Taints are Tolerations are:</p>\n<ul>\n<li>dedicated nodes - if you have different types of nodes, dedicated for different workloads in the same cluster, you can use Taints and Tolerations to make sure each Pods are scheduled on the right nodes.</li>\n<li>nodes with special hardware - if you a set of nodes that have some special hardware (for example, GPUs) you can use Taints and Tolerations to make sure that only the Pods that need that hardware are scheduled on these nodes</li>\n<li>eviction - you can implement a logic that adds a <code>NoExecute</code> taints on a Node if certain conditions are met.\nThis will evict all Pods from that Node and reschedule them on different Nodes.\nKubernetes uses Taints with the <code>NoExecute</code> effect to make sure that no Pods are scheduled on Nodes that are consider not ready or have some sort of hardware issues (disk, CPU) or networking problems (unreachable nodes, etc.)\nAlso, if you want to remove a Node, you can add a <code>NoExecute</code> taint before actually removing the Node, to ensure a smooth transition of all Pods to the other Nodes.</li>\n</ul>\n<h3><a href=\"https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\">ResourceLimits</a></h3>\n<p>For each Pod, you can specify how many resources a container needs (requests) and what is the maximum (limit) that the container can get.</p>\n<p>The Kube scheduler will use the information from <code>requests</code> when scheduling a Pod onto a Node.</p>\n<p>The Kubelet will enforce that no container gets more resources than its <code>limit</code>.</p>\n<p>Specifying Resource limits in Kubernetes is optional but highly advisable. If you don’t set resource limits for a container and that container has a problem like a memory leak, it will use all the Node&#39;s memory and starve all other workloads on that Node out of memory.</p>\n<h2>Summary</h2>\n<p>This is all for part two.\nI hope you enjoyed it and learned something new.</p>\n<p>The series continues with <a href=\"/post/demystifying-the-kubernetes-iceberg-3/\">Part 3</a>.</p>\n<p>If you don’t want to miss it, you can follow me on <a href=\"https://twitter.com/a_sankov\">Twitter</a> or <a href=\"https://www.linkedin.com/in/asankov/\">LinkedIn</a>.</p>\n"
            }
        };
    </script>
</body>
</html>
                