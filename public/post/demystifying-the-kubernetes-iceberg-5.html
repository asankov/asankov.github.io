
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Demystifying the Kubernetes Iceberg: Part 5</title>
    <meta name="description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-5" />
    <meta property="og:title" content="Demystifying the Kubernetes Iceberg: Part 5" />
    <meta property="og:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />
    <meta property="og:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-5" />
    <meta property="twitter:title" content="Demystifying the Kubernetes Iceberg: Part 5" />
    <meta property="twitter:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />
    <meta property="twitter:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <link rel="stylesheet" href="/index.css">
    <script type="module" crossorigin src="/assets/index-xxxxxxxx.js"></script> <!-- Placeholder for Vite's main JS bundle -->
</head>
<body>
    <div id="root"></div>
    <script>
        // Store pre-rendered content for client-side hydration
        window.__PRELOADED_STATE__ = {
            post: {
                slug: "demystifying-the-kubernetes-iceberg-5",
                title: "Demystifying the Kubernetes Iceberg: Part 5",
                description: "Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant.",
                date: "2022-06-12T00:00:00.000Z",
                content: "<p>This is the fifth article of the &quot;Demystifying the Kubernetes Iceberg&quot; series.\nMy goal for this series is to explain all concepts mentioned in the “Kubernetes Iceberg” diagram by <a href=\"https://flant.com/\">Flant</a>.</p>\n<p>This is the iceberg:</p>\n<p><img src=\"/images/kubernetes-iceberg.png\" alt=\"The Kubernetes Iceberg meme\"></p>\n<p>In this article, we will continue with Tier 5 of the iceberg.\nWe started with the <a href=\"/post/demystifying-the-kubernetes-iceberg-4/\">first part of Tier 5 last week</a>, and today we will pick up where we left off.</p>\n<p>You can find the others articles here:</p>\n<ul>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-1/\">Part 1</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-2/\">Part 2</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-3/\">Part 3</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-4/\">Part 4</a></li>\n</ul>\n<p>I will publish one article each week until I complete the whole iceberg.</p>\n<h2>Tier 5, Part 2</h2>\n<h3>Multitenancy</h3>\n<p>Multitenancy in Kubernetes is not an easy problem to solve.\nPart of the reason is that Kubernetes came out from Google, and multitenancy is usually not a necessity in the realm of a single company.</p>\n<p>However, people have been trying to do multi-tenant Kubernetes for a while now.\nThere are some solutions and blueprints to how it could be done.\nI would try to list the main ways to do that, with their pros and cons.</p>\n<p>In these examples, we will assume that we are still talking about a single company, and the tenants are different teams inside that company.</p>\n<h4>Multitenancy with namespaces</h4>\n<p>One way to do multitenancy in Kubernetes is via namespaces.</p>\n<p>Each team(tenant) gets its own namespace.\nUsing <a href=\"/post/demystifying-the-kubernetes-iceberg-4/#rbac\">RBAC</a> we can assign permissions to the team members for only the namespace of their team.</p>\n<p><strong>Pros:</strong></p>\n<ul>\n<li>Easy to do.\nA new team comes - create a new namespace.</li>\n<li>Does not require any additional software for maintenance.</li>\n<li>Easy for a centralized team to enforce some sets of rules and standards for all teams using this cluster.</li>\n</ul>\n<p><strong>Cons:</strong></p>\n<ul>\n<li>Isolation is not ideal.<ul>\n<li>User access is regulated via RBAC, but workloads (Pods) can still see other and talk to each other even when in separate namespaces.</li>\n<li>Workloads will also run on a shared infrastructure, so a problem like a memory leak or container escape in one workload can easily affect other tenants’ workloads.</li>\n<li>Some cluster-wide resources (like operators) can conflict with each other because there are many teams but just one cluster, and there can be only one such resource in a cluster.</li>\n</ul>\n</li>\n<li>Requires additional management<ul>\n<li>Since provisioning multiple clusters by hand will become pretty tedious quickly, this solution will almost always require an additional management layer on top of all your Kubernetes clusters.\nThis could be any of the cloud providers (AWS EKS, Azure AKS, GCP) or some self-hosted solution like VMware Tanzu TKG or Rancher.</li>\n</ul>\n</li>\n</ul>\n<h4>Multitenancy via different clusters</h4>\n<p>Another way to do multitenancy in Kubernetes is to… not do multitenancy in Kubernetes.\nJust provide a different cluster for every tenant.</p>\n<p>Pros:</p>\n<ul>\n<li>Good isolation.\nA problem in one tenant&#39;s workload will not affect other tenants.</li>\n<li>Good autonomy.\nEach team is the owner of their cluster, and they can do whatever they like.</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Expensive.\nEach cluster has some resource overhead for the master nodes and other system components.\nWith this approach, this cost is multiplied by the number of teams.</li>\n<li>It is harder for a centralized authority to enforce the same set of rules to every cluster in the organization because there will be many clusters.</li>\n</ul>\n<h4>Multitenancy via virtual clusters</h4>\n<p>This approach aims to get the best of both worlds.\nIt involves deploying an additional layer that abstracts your namespaces and makes them look like separate clusters from the outside.</p>\n<p>An existing solution is the <a href=\"https://github.com/loft-sh/vcluster\">vcluster</a> project by <a href=\"https://loft.sh/\">loft.sh</a>.</p>\n<p>In short, how it works is that it gives access to a user to a cluster, which is just a namespace in the main cluster.\nThe user can deploy workloads there, but they will never be scheduled because they will actually be scheduled in the main namespace in the main cluster.</p>\n<p>That way, it gives the tenants autonomy while still providing an easy way for cluster admins to enforce centralized rules and policies.</p>\n<h3>Cert-manager</h3>\n<p><a href=\"https://cert-manager.io/\">cert-manager</a> is a X.509 certificate controller for Kubernetes.</p>\n<p>It can be configured to obtain certificates from public Issuers (such as <a href=\"https://letsencrypt.org/\">Let&#39;s Encrypt</a>) or private ones.\nIt is also responsible for keeping the certificates up-to-date, so it will attempt to renew any expiring ones.</p>\n<p>Nowadays, using TLS for public connections is mandatory, but it is also recommended even for private service-to-service communication.\n<code>cert-manager</code> is a valuable project that can help you a lot in being more secure in that aspect.</p>\n<h3>Certificate renewal</h3>\n<p>Certificate renewal is the process of renewing your SSL certificates.\nThis is necessary because each SSL certificate has an expiration date, after which it is not valid.</p>\n<p>In the past, certificate issuers used to issue certificates with huge validity periods (5 years, for example).\nThis is now considered a bad practice, and browsers will usually reject certificates with validity bigger than 1 or 2 years.</p>\n<p>Let&#39;s Encrypt issues certificates valid only for three months.\nThis is to encourage the use of automation when renewing certificates.</p>\n<p>If you are using <code>cert-manager</code>, it can automatically handle the certificate renewal process.</p>\n<h3>cluster-autoscaler</h3>\n<p><a href=\"https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler\">Kubernetes cluster-autoscaler</a> is a tool that automatically adjusts the size of the cluster (number of nodes) based on several factors, such as node utilization and failing pods.</p>\n<p>It makes sure that there is a place for all Pods to run while at the same time it is not using more nodes than it could be.</p>\n<p>There is an available implementation for most major cloud providers such as <a href=\"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/aws/README.md\">AWS</a>, <a href=\"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/azure/README.md\">Azure</a> or an <a href=\"https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/cloudprovider/externalgrpc/README.md\">external out-of-tree one</a>.</p>\n<h3>Egress gateway</h3>\n<p>Egress gateway is a gateway for the outgoing traffic.</p>\n<p>An egress gateway allows you to limit the outgoing traffic of your workloads.\nThis could be a useful security feature for preventing an attacker from making malicious network connections to the outside world.</p>\n<p>This is not a native Kubernetes concept, but it is implemented by some Kubernetes network implementations (CNI), such as <a href=\"https://projectcalico.docs.tigera.io/about/about-kubernetes-egress#:~:text=enterprise%20deployment%20scenarios.-,Egress%20gateways,-Another%20approach%20to\">Calico</a> and <a href=\"https://docs.cilium.io/en/stable/gettingstarted/egress-gateway/\">Cilium</a> or by service meshes like <a href=\"https://istio.io/latest/docs/tasks/traffic-management/egress/egress-gateway/\">Istio</a>.</p>\n<h3>descheduler</h3>\n<p>The <a href=\"https://github.com/kubernetes-sigs/descheduler\">descheduler</a> is a Kubernetes component that is responsible for descheduling workloads.</p>\n<p>That can happen for a variety of reasons:</p>\n<ul>\n<li>Some nodes are under or overutilized.</li>\n<li>Taints or labels are added to or removed from nodes, and pod/node affinity requirements are not satisfied anymore.</li>\n<li>Some nodes failed, and their pods moved to other nodes.</li>\n<li>New nodes are added to clusters.</li>\n</ul>\n<p>After the descheduler has descheduled the pods, they are back to the scheduler, and it is his responsibility to reschedule them again.</p>\n<h3>Custom Resources validation and conversion</h3>\n<p><code>CustomResourceDefinitions</code> are a way to define custom resources and thus extend the Kubernetes API.</p>\n<p>Each Custom Resource Definitions defines its fields via an OpenAPI spec.\nFor example, a <code>User</code> CRD can have the following API spec:</p>\n<pre><code class=\"language-yaml\">schema:\n  openAPIV3Schema:\n    type: object\n    properties:\n      spec:\n        type: object\n        properties:\n          id:\n            type: string\n          name:\n            type: string\n</code></pre>\n<p>We see that the <code>User</code> has two properties - <code>id</code> and <code>name</code>, both of type <code>string</code>.</p>\n<p>The Open API spec of the resource can be extended to specify additional validation for the fields.\nFor example, you could define required properties via the <code>required</code> parameter or specify a regex validator for the string values via the <code>pattern</code> parameter:</p>\n<pre><code class=\"language-yaml\">id:\n  type: string\n  required: true\n</code></pre>\n<p>The validation for these fields will be performed by the Kubernetes API automatically, and if a user tries to create an object that violates these rules, the request will fail.</p>\n<h3>etcd cluster management</h3>\n<p><code>etcd</code> is a distributed key-value store.\nBy default, Kubernetes uses etcd as the place where all data is persisted.\nFor example, when we create a Pod resource, that gets persisted into etcd.</p>\n<p>Since <code>etcd</code> is by-design, a distributed data store, most production environments run multiple etcd instances.\nThis way, even if one of them dies, our data will be safe in the other ones.</p>\n<p>In order to work together, the <code>etcd</code> instance need to be aware of one another and be able to communicate with each other.\nThis requires some amount of configuration when starting the cluster.</p>\n<p>Most of the important things to know when configuring an etcd cluster, like service discovery, DNS and TLS configuration, etc., are described <a href=\"https://etcd.io/docs/v3.4/op-guide/clustering/\">here</a>.</p>\n<p>Finally, a good security measure for <code>etcd</code> is to run it on dedicated master nodes, which are not publicly available, and are configured via Network Policies to be only accessible to the API server.\nThis is because we should never interact directly with etcd, only through the API server.\nAlso, if an attacker got access to etcd, they could damage our cluster (for example, by deleting resources.)</p>\n<h3>Kubernetes Upgrade</h3>\n<p>Upgrading your Kubernetes cluster is an important thing in the cluster lifecycle.\nNew Minor Kubernetes versions (v1.XX.0) are released every four months, with patch versions (v1.24.XX) released more often to address bugs and security vulnerabilities.</p>\n<p>New Kubernetes versions provide new features, and old Kubernetes versions eventually reach End-Of-Life, and support for them is dropped.\nYou should always be running a supported Kubernetes version.</p>\n<p>For people running managed Kubernetes (e.g., EKS, AKS, GKE, Rancher, etc.), upgrading your Kubernetes cluster can be as simple as switching a value in a drop-down menu in the cloud-provider UI.</p>\n<p>That is not the case, if you are managing your Kubernetes cluster on your own.\nIf so, you would need to upgrade your cluster manually.\nBy &quot;manually,&quot; I mean there is a helpful tool that manages most of the heavy lifting, but you will still be responsible for using it properly.</p>\n<p>That tool is called <a href=\"https://github.com/kubernetes/kubeadm\"><code>kubeadm</code></a> and helps not only for upgrading a cluster but also for setting it up.</p>\n<p>The process of upgrading a cluster is described in detail <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\">here</a>, but the basic steps to follow are these:</p>\n<ul>\n<li>BACKUP your data!</li>\n<li>upgrade your master node one by one</li>\n<li>upgrade your CNI</li>\n<li>upgrade your worker nodes one by one</li>\n</ul>\n<h3>IaC for Grafana (dashboards, data sources)</h3>\n<p>IaC stands for Infrastructure as Code.\nThis paradigm involves describing your infrastructure in text files (code) and having a tool that will provision/destroy resources based on the contents of this file and the changes to it.\nSuch tools are <a href=\"/post/demystifying-the-kubernetes-iceberg-3/#terraform\">Terraform</a>, Pulumi and others.</p>\n<p>Most IaC tools like Terraform can provision absolutely everything given that there is a provider for it.\nA provider is an implementation that provisions infrastructure based on your code.</p>\n<p>There is such provider for <a href=\"https://registry.terraform.io/providers/grafana/grafana/latest/docs\">Grafana</a>.\nUsing it, you can describe your dashboards and data sources as code and get these automatically created for you by your IaC tool.</p>\n<p>For example, this is a Terraform code snippet that will create a Grafana dashboard based on the <code>grafana-dashboard.json</code> JSON file:</p>\n<pre><code class=\"language-terraform\">resource &quot;grafana_dashboard&quot; &quot;metrics&quot; {\n  config_json = file(&quot;grafana-dashboard.json&quot;)\n}\n</code></pre>\n<p>It is also possible to manually configure your dashboards by hand and then export the Terraform code for them.\nThis way, you get the best of both worlds - manual configuration maintained by your IaC tool.</p>\n<h3>Advanced control plane configuration</h3>\n<p>The Kubernetes control plane and its components (<code>api-server</code>, <code>controller-manager</code>, <code>scheduler</code>, and <code>etcd</code>) support some customization depending on the user&#39;s needs.</p>\n<p>This customization is different for each component, but if I have to point out some of the most important things for each one:</p>\n<h4><a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/\">API server</a></h4>\n<ul>\n<li>audit log configuration - max size, the place to be stored, retention, etc.</li>\n<li>TLS certificates</li>\n<li>configuration of the leader-elect algorithm</li>\n<li>enabling and disabling optional features</li>\n<li>metrics</li>\n</ul>\n<h4><a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/\">Controller manager</a></h4>\n<ul>\n<li>TLS certificates</li>\n<li>configuration of the leader-elect algorithm</li>\n<li>enabling and disabling optional features</li>\n<li>metrics</li>\n</ul>\n<h4><a href=\"https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/\">Scheduler</a></h4>\n<ul>\n<li>TLS certificates</li>\n<li>configuration of the leader-elect algorithm</li>\n<li>enabling and disabling optional features</li>\n<li>metrics</li>\n</ul>\n<h4><a href=\"https://etcd.io/docs/\">etcd</a></h4>\n<ul>\n<li>TLS certificates</li>\n<li>metrics</li>\n<li>clustering configuration</li>\n</ul>\n<h3>Customizable monitoring for all Kubernetes objects</h3>\n<p>Fully monitoring all your Kubernetes objects is vital in order to have complete visibility of the state of your cluster and be able to act accordingly when needed.</p>\n<p>Kubernetes provides you with two ways to monitor your resources.\nThe first one is the <strong>resource metrics pipeline</strong>, which gives you a limited set of metrics related to cluster components.\nThey are saved in a short-term in-memory metrics server and exposed via the <code>metrics.k8s.io</code> API or via the <code>kubectl top</code> utility.</p>\n<p>The second one is the <strong>full metrics pipeline</strong>, which is more sophisticated and gives you more metrics to work with.\nThese are exposed by implementing either of the <code>custom.metrics.k8s.io</code> or <code>external.metrics.k8s.io</code> APIs.</p>\n<p>A CNCF-supported implementation of these APIs is <a href=\"/post/demystifying-the-kubernetes-iceberg-3/#prometheus\">Prometheus</a>.</p>\n<h3>Long-term Prometheus</h3>\n<p><a href=\"/post/demystifying-the-kubernetes-iceberg-3/#prometheus\">Prometheus</a> is an open-source monitoring and alerting toolkit.</p>\n<p>It is used for metrics collection and aggregation.post</p>\n<p>It can also be integrated with <a href=\"https://prometheus.io/docs/prometheus/latest/storage/#local-storage\">local</a> or <a href=\"https://prometheus.io/docs/prometheus/latest/storage/#remote-storage-integrations\">remote</a> file storage to achieve bigger data retention.</p>\n<p>The file storage options are configured via the <a href=\"https://prometheus.io/docs/prometheus/latest/storage/#operational-aspects\"><code>--storage.XXX</code> command line arguments</a>.</p>\n<p>When using long-term storage, it is advisable to lower the number of time series scrapes in order to save space (and also because you probably would not care about second-by-second of your 6-month-old metrics).</p>\n<h3>Prometheus Query Caching</h3>\n<p>Prometheus queries are written in <a href=\"/post/demystifying-the-kubernetes-iceberg-3/#promql\">PromQL</a>.</p>\n<p>PromQL is quite powerful and can do many things like summing, averages, aggregation, etc.\nThese are heavy operations that, if executed on a large dataset, can take a significant amount of CPU and memory to complete.</p>\n<p>That is why Prometheus front-ends like Grafana (and Prometheus itself) support query caching, e.g., saving the results of a given query for some time, and if another user runs the same query on the same data, it will return the cached results.\nThis speeds up the process of fetching the data and avoids unnecessary computations.</p>\n<p>Of course, this is a trade-off because in a real-time system like Prometheus, the data changes by the second, so we cannot set too big a cache TTL without risking showing our users outdated data.</p>\n<h3>Ingress Monitoring</h3>\n<p>The <a href=\"/post/demystifying-the-kubernetes-iceberg-1/#ingress\">ingress</a> manages the external traffic coming into the cluster.</p>\n<p>It is an integral part of our system - if the ingress is down or it cannot scale, our whole system will be blocked because all requests go through the ingress before getting into it.</p>\n<p>That is why it&#39;s important to have monitoring in place so that we know at any given time what traffic flows into our system, what the latencies are, are there any problems, etc.</p>\n<p>Since the <code>Ingress</code> resource is an abstract one, all functionality is implemented by the controllers.\nSo is the monitoring.\nDifferent ingress controller implementations provide different monitoring constructs, but they all achieve the same results.</p>\n<p>For example, if you are running the <a href=\"https://docs.nginx.com/nginx-ingress-controller\">NGINX Ingress controller</a>, you can enable metrics via the <code>-enable-prometheus-metrics</code> flag, and then you can consume the metrics listed <a href=\"https://docs.nginx.com/nginx-ingress-controller/logging-and-monitoring/prometheus/\">here</a>.</p>\n<h3>Ingress autoscaling</h3>\n<p>After enabling ingress metrics, you can take it to the next level and autoscale your ingress based on these metrics.\nFor example, spin up new instances once you have a traffic peak and the network latency starts growing.</p>\n<p>It&#39;s vital that your ingress is scaled appropriately because this is the entry point of the traffic to your application.\nIf not scaled correctly, the ingress could be a bottleneck which can slow your application and cause frustration to your users.</p>\n<p>Just like the monitoring, autoscaling is also provided by the controller implementations.</p>\n<p>For example, if you are running the <a href=\"https://docs.nginx.com/nginx-ingress-controller\">NGINX Ingress controller</a>, you can configure autoscaling via the KEDA autoscaler that consumes the nginx metrics.</p>\n<p>This object configures KEDA to autoscale the NGINX Pods if the average connections for a minute are more than 100.</p>\n<pre><code class=\"language-yaml\">apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n name: nginx-scale\nspec:\n scaleTargetRef:\n   kind: Deployment\n   name: main-nginx-ingress\nminReplicaCount: 1\nmaxReplicaCount: 20\ncooldownPeriod: 30\npollingInterval: 1\ntriggers:\n- type: prometheus\n   metadata:\n     serverAddress: http://prometheus-server\n     metricName: nginx_connections_active_keda\n     query: |\n       sum(avg_over_time(nginx_ingress_nginx_connections_active{app=&quot;main-nginx-ingress&quot;}[1m]))\n     threshold: &quot;100&quot;\n</code></pre>\n<h3>Resource sharing</h3>\n<p>Kubernetes runs multiple workloads (Pods) on the same physical/virtual Node.\nThis means that these workloads will share the underlying host resources.</p>\n<p>Kubernetes provides two constructs to control how these resources are shared - resource limits and resource requests.</p>\n<p>We discussed resource limits in <a href=\"/post/demystifying-the-kubernetes-iceberg-2\">Part 2</a> of this series, but now I will go into more detail about the limits and requests and the difference between the two.</p>\n<p>The <em>requests</em> show how many minimum resources the workload will need.\nThe kubelet will use this information to find a proper Node to schedule the Pod (one with at least that many resources as the requests for the Pod).\nA Pod can go under or over the requested resources.</p>\n<p>The <em>limits</em> show how many maximum resources the workload will need.\nKubernetes will not allow a resource to use more than its limits.\nThis limit is enforced by the container runtime, and a resource will not be allowed to exceed it.</p>\n<h3>Dynamic StorageClass provisioning</h3>\n<p>When you create a Volume in Kubernetes, you specify its <code>StorageClass</code>.\nThe <code>StorageClass</code> shows some properties of the volume, e.g., its quality-of-service levels, its backup policies, or other policies determined by the cluster administrator.</p>\n<p>You are not limited to the number of <code>StorageClasses</code> you can use, and you can even define your own <code>StorageClasses</code> by creating a new Kubernetes resource of that type.</p>\n<p>This is a sample <code>StorageClass</code>:</p>\n<pre><code class=\"language-yaml\">apiVersion: storage.k8s.io/v1\nkind: StorageClass\nprovisioner: kubernetes.io/aws-ebs\nmetadata:\n  name: standard\nreclaimPolicy: Retain\nallowVolumeExpansion: true\nvolumeBindingMode: Immediate\nparameters:\n  type: gp2\nmountOptions:\n  - debug\n</code></pre>\n<p>From its properties, we see that volumes from this class are provisioned by AWS EBS; it has the GP2 type, reclaim policy of <code>Retain</code>, etc.\nThe <code>parameters</code> depend on the provisioner, e.g., AWS EBS has one set of parameters that make sense for this provisioner, GCE PD has other parameters, and so on.</p>\n<h2>Summary</h2>\n<p>This is all for part five.</p>\n<p>In the last two article, we managed to demystify the biggest tier of the iceberg.\nThe next ones are smaller, but are getting more and more specific.\nI don&#39;t know about you, but I can&#39;t wait to dive into them.</p>\n<p>The series continues with <a href=\"/post/demystifying-the-kubernetes-iceberg-6\">Part 6</a>.</p>\n<p>If you don’t want to miss it, you can follow me on <a href=\"https://twitter.com/a_sankov\">Twitter</a> or <a href=\"https://www.linkedin.com/in/asankov/\">LinkedIn</a>.</p>\n"
            }
        };
    </script>
</body>
</html>
                