
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/favicon.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Demystifying the Kubernetes Iceberg: Part 6</title>
    <meta name="description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-6" />
    <meta property="og:title" content="Demystifying the Kubernetes Iceberg: Part 6" />
    <meta property="og:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />
    <meta property="og:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://asankov.dev/post/demystifying-the-kubernetes-iceberg-6" />
    <meta property="twitter:title" content="Demystifying the Kubernetes Iceberg: Part 6" />
    <meta property="twitter:description" content="Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant." />
    <meta property="twitter:image" content="https://github.com/asankov/asankov.github.io/blob/main/preview.png?raw=true" />

    <link rel="stylesheet" href="/index.css">
    <script type="module" crossorigin src="/assets/index-xxxxxxxx.js"></script> <!-- Placeholder for Vite's main JS bundle -->
</head>
<body>
    <div id="root"></div>
    <script>
        // Store pre-rendered content for client-side hydration
        window.__PRELOADED_STATE__ = {
            post: {
                slug: "demystifying-the-kubernetes-iceberg-6",
                title: "Demystifying the Kubernetes Iceberg: Part 6",
                description: "Kubernetes is like an iceberg. You learn the basics, only to see there is a lot more to learn. The more you learn, the more you see there is to know. This series of articles explains all the concepts listed in the "Kubernetes Iceberg" diagram by Flant.",
                date: "2022-06-27T00:00:00.000Z",
                content: "<p>This is the sixth article of the &quot;Demystifying the Kubernetes Iceberg&quot; series.\nMy goal for this series is to explain all concepts mentioned in the “Kubernetes Iceberg” diagram by <a href=\"https://flant.com/\">Flant</a>.</p>\n<p>This is the iceberg:</p>\n<p><img src=\"/images/kubernetes-iceberg.png\" alt=\"The Kubernetes Iceberg meme\"></p>\n<p>In this article, we will continue with Tier 6 of the iceberg.</p>\n<p>This article is a week late, because last week I attended <a href=\"https://oscal.openlabs.cc/\">OSCAL 2022</a> at Tirana, Albania and did not have time to write.</p>\n<p>You can find the others articles here:</p>\n<ul>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-1/\">Part 1</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-2/\">Part 2</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-3/\">Part 3</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-4/\">Part 4</a></li>\n<li><a href=\"/post/demystifying-the-kubernetes-iceberg-5/\">Part 5</a></li>\n</ul>\n<p>I will publish one article each week until I complete the whole iceberg.</p>\n<h2>Tier 6</h2>\n<h3>Monitoring underlying infrastructure</h3>\n<p>Even if you are deploying your application into a Kubernetes cluster, you still need to care about the underlying infrastructure hosting this cluster.\nUsually, that would be a set of either virtual machines or bare-metal hosts.</p>\n<p>The metrics that you need to care about are the same ones that you would monitor if you were not using Kubernetes:</p>\n<ul>\n<li>machine health</li>\n<li>CPU</li>\n<li>memory</li>\n<li>disk I/O</li>\n</ul>\n<p>On top of that, you can add the Kubernetes and container-specific metrics:</p>\n<ul>\n<li>CPU per container</li>\n<li>memory per container</li>\n<li>disk I/O per container</li>\n<li>number of running pods and pod state</li>\n<li>numbers of running nodes and their statuses</li>\n</ul>\n<p>A lot of the Kubernetes-specific metrics will be provided to you by the <code>kube-state-metrics</code> component.\nIn contrast, the non-Kubernetes-specific ones must be provided by the infrastructure provider.\nThat can be the cloud provider who provided the underlying machines or if you are running on-prem - the management software you use to manage your machines.</p>\n<p>For more info and good practices about metrics, you can check <a href=\"https://www.datadoghq.com/blog/monitoring-kubernetes-performance-metrics/\">this great article</a> by Datadog.</p>\n<h3>Terraform-managed infrastructure</h3>\n<p><a href=\"/2022/05/29/demystifying-the-kubernetes-iceberg-3/#terraform\">Terraform</a> is an open-source IaC tool.\nIt allows you to describe your infrastructure in text files and have it created for you by Terraform.\nEach change in your infrastructure goes through a change in the Terraform files.</p>\n<p>You can use Terraform to provision a Kubernetes cluster in any of the major cloud providers (e.g., <a href=\"https://aws.amazon.com/\">AWS</a>, <a href=\"https://azure.microsoft.com/en-us/\">Azure</a>, <a href=\"https://cloud.google.com/\">GCP</a>, etc.) via the Terraform providers for that cloud.</p>\n<p>You can also use Terraform to provision the underlying infrastructure (virtual machines) and use that to spin up your own Kubernetes clusters (for example, via kubeadm).</p>\n<p>The latter is less common because if you already use a cloud provider to manage your infrastructure, why not use the complete package and let it manage your Kubernetes clusters. But it has its use-cases, for example, if you are running multi-cloud and want to provision VMs in many clouds but then create clusters on top of them in a unified manner.</p>\n<h3>Cost analysis (cloud provider resources)</h3>\n<p>Cost analysis is an important topic in today&#39;s world, where many of our workloads are running in public clouds (like AWS), where we are billed by the hour.</p>\n<p>We are often over-provisioning infrastructure and are paying for more than we are using.\nThere are many reasons for this.\nIf you are a well-funded early-stage startup, your main goal is to build a good product as quickly as possible and reach a product-market fit.\nIn this case, you usually don&#39;t care about your cloud provider bill.\nAnother reason is that the people using the infra are not paying for it, so when they create yet another Kubernetes cluster or yet another virtual machine, they don&#39;t realize (or don&#39;t care) about the cost implications to the company.\nAlso, developers are often lazy and forget to stop a VM or down-scale.</p>\n<p>That is why there are a lot of companies focused on building product that helps you measure your cloud bill and find ways to reduce it.</p>\n<p>Such are <a href=\"https://www.kubecost.com/\">Kubecost</a>, <a href=\"https://www.cloudzero.com/blog/kubecost-alternatives\">CloudZero</a> and native ones provided by your cloud provider (<a href=\"https://aws.amazon.com/aws-cost-management/aws-cost-explorer/\">AWS Cost Explorer</a>, <a href=\"https://azure.microsoft.com/en-us/services/cost-management/\">Microsoft Cost Management</a>, <a href=\"https://cloud.google.com/cost-management\">GCP Cost Management</a>).</p>\n<h3>CNI (Cilium, Calico, flannel, integration with cloud provider VPCs)</h3>\n<p><a href=\"https://github.com/containernetworking/cni\">CNI</a> stands for Container Network Interface.\nThe CNI implementation is responsible for managing the Kubernetes networking, e.g., configuring the network, provisioning IPs, and maintaining connectivity between the hosts.</p>\n<p>The container runtime communicates with the CNI, so all the configuration is dynamic, changing when pods are created or deleted.</p>\n<p>Some CNIs give you additional features, such as <a href=\"/post/demystifying-the-kubernetes-iceberg-4/#networkpolicy\">Network Policies</a>.</p>\n<p>There are different implementations of a CNI.\nThese include (but are not limited to):</p>\n<h4>Cilium</h4>\n<p><a href=\"https://cilium.io/\">Cilium</a> is an open-source CNCF incubator project that provides networking, security, and observability powered by <a href=\"https://ebpf.io/\">eBPF</a>.</p>\n<p>It is initially created by <a href=\"https://isovalent.com/\">Isovalent</a>.</p>\n<h4>Calico</h4>\n<p><a href=\"https://www.tigera.io/project-calico/\">Calico</a> is a partially open-source project developed and maintained by <a href=\"https://www.tigera.io/\">Tigera</a>.</p>\n<p>It supports both Linux and Windows and also non-Kubernetes workloads.</p>\n<h4>flannel</h4>\n<p><a href=\"https://github.com/flannel-io/flannel\">Flannel</a> is an open-source project that provides Layer 3 networking.</p>\n<p>It works by deploying the <code>flanneld</code> daemon on each node and allocating subnets for the nodes.</p>\n<h4>integration with cloud provider VPCs</h4>\n<p>When using managed Kubernetes (e.g., <a href=\"https://aws.amazon.com/eks/\">EKS</a>, <a href=\"https://azure.microsoft.com/en-us/services/kubernetes-service/\">AKS</a>, <a href=\"https://cloud.google.com/kubernetes-engine\">GKE</a>, etc.), you get the underlying Kubernetes infrastructure managed for you by the cloud provider.\nYou also get the Kubernetes managed for you (installation, upgrade, etc.).</p>\n<p>Another thing that the cloud provider can manage for you is the networking side.\nCloud providers allow you to create <a href=\"https://aws.amazon.com/vpc/\">VPCs - Virtual Private Clouds</a>.\nThese are isolated environments that are fully encapsulated, and you have the full power to define the level of visibility of a resource inside a VPC (or the whole VPC) to the outside world.\nFor example, you could create a VPC for your test environment, which is completely hidden from the outside world and only accessible through a jump box or VPN. That way, the only people that can access your test environment are those that have access to the VPC, e.g., your developers, but you would be safe from that leaking to the outside world or being indexed by a search engine.</p>\n<p>The same concepts can be applied to a Kubernetes cluster.\nCloud providers allow you to have a cluster isolated inside its own VPC so that all the private components (etcd, api server) stay private.\nOnce there, you can take steps to explicitly expose the parts that need to be public (the ingress of your app) via a gateway that is outside of the VPC but has access to it.</p>\n<p>AWS also has a CNI plugin whose implementation is tightly coupled to the VPC configuration - <a href=\"https://github.com/aws/amazon-vpc-cni-k8s\">https://github.com/aws/amazon-vpc-cni-k8s</a></p>\n<h3>sysctl</h3>\n<p><a href=\"https://man7.org/linux/man-pages/man8/sysctl.8.html\"><code>sysctl</code></a> is a Linux utility that is used to modify kernel parameters at runtime.</p>\n<p>Since most Kubernetes clusters run on Linux nodes, <code>sysctl</code> can be used on those nodes.</p>\n<p>One can perform <code>sysctl</code> commands from a Pod. In that case, the commands are split into 2 - safe and unsafe.\nSafe commands affect only the Pod and do not affect the other Pods or the node. Unsafe commands can affect the other Pods and the Node.</p>\n<p>By default, Kubernetes enables all safe <code>sysctl</code> commands, but the unsafe ones need to be specifically enabled by a cluster admin.\nThis is done by the <code>--allowed-unsafe-sysctls</code> kubelet flag.</p>\n<p>Pod sysctl parameters can be configured via the <code>spec.securityContext.sysctls</code> parameters.\nFor example, the following Pod spec configures this Pod with the sysctl parameter <code>kernel.shm_rmid_forced</code> set to <code>0</code>:</p>\n<pre><code class=\"language-yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: sysctl-example\nspec:\n  securityContext:\n    sysctls:\n      - name: kernel.shm_rmid_forced\n        value: &quot;0&quot;\n</code></pre>\n<p>In this scenario, Kubernetes does not differentiate between safe and unsafe sysctls, so be careful what you set.</p>\n<p>If you want to configure sysctls on a node, you would have to do that manually or run a privileged <code>DaemonSet</code> that will set the sysctl parameters for each node.</p>\n<h3>Control plane maintenance windows</h3>\n<p>When using managed Kubernetes (e.g., <a href=\"https://aws.amazon.com/eks/\">EKS</a>, <a href=\"https://azure.microsoft.com/en-us/services/kubernetes-service/\">AKS</a>, <a href=\"https://cloud.google.com/kubernetes-engine\">GKE</a>, etc.), the cloud provider is responsible for managing the underlying infrastructure, and the cluster itself includes version upgrades.</p>\n<p>If you haven&#39;t upgraded your cluster in a while, the cloud provider can do a forced upgrade.\nFor example, this can happen if the cloud provider no longer supports the Kubernetes version you are running, so you need to move to a newer one. Some cloud providers also support auto upgrades to the latest version.</p>\n<p>Sometimes the cloud provider lets you define maintenance windows so that this update is not disruptive to your business.\nFor example, if you are a retail business, you might want to set this window to when there are the least number of customers.\nOr, if your customers are based in Europe, you might want to put the maintenance window during the night in Europe so that your customers are not affected.\nAll of this is business-specific, but you get the point.</p>\n<p>You can check the maintenance windows docs for <a href=\"https://docs.microsoft.com/en-us/azure/aks/planned-maintenance\">AKS</a> and <a href=\"https://cloud.google.com/kubernetes-engine/docs/concepts/maintenance-windows-and-exclusions\">GKE</a>.</p>\n<h3>Service Mesh (Istio)</h3>\n<p>A service mesh is a dedicated infrastructure layer that can be used to transparently add capabilities to your application.\nThese include monitoring, observability, TLS termination, authentication/authorization, load balancing, etc.</p>\n<p>This layer stands between your application and your customers.\nIn Kubernetes, it is often implemented as sidecar containers.\nThis means that Kubernetes will create an additional container for each of your existing containers.\nThis container will hold the service mesh logic and serve as both ingress and egress proxy for the actual workload container.\nThat way, you can implement TLS termination there, and your application would not bother with TLS logic, but at the same time, your system will be TLS secure.</p>\n<h4>Istio</h4>\n<p><a href=\"https://istio.io/\">Istio</a> is an open-source implementation of Service mesh.\nIt works by deploying sidecar containers.</p>\n<p>It is developed and maintained by Google.</p>\n<p>All the logic for the Istio service mesh is configured via <a href=\"/post/demystifying-the-kubernetes-iceberg-2/#crd\">CRDs</a> which you apply to your cluster.\nFor example, these 2 CRDs configure A/B testing for a service that has 2 versions with a 60/40 distribution between the versions:</p>\n<pre><code class=\"language-yaml\">apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: test-vs\nspec:\n  gateways:\n    - test-gateway\n  hosts:\n    - &quot;*&quot;\n  http:\n    - route:\n        - destination:\n            host: test-svc\n            subset: &quot;v1&quot;\n          weight: 60\n        - destination:\n            host: test-svc\n            subset: &quot;v2&quot;\n          weight: 40\n---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: test-destination-rule\nspec:\n  host: test\n  subsets:\n    - name: &quot;v1&quot;\n      labels:\n        version: &quot;v1&quot;\n    - name: &quot;v2&quot;\n      labels:\n        version: &quot;v2&quot;\n</code></pre>\n<p>Another popular implementations of a Service mesh is <a href=\"https://linkerd.io/\">Linkerd</a>.</p>\n<h3>Network request tracing</h3>\n<p>Tracing the network requests coming in and out of your Kubernetes cluster can be a handy thing to do when debugging a network problem.\nThis can be implemented in several ways.</p>\n<h4>Service meshes</h4>\n<p>Some service meshes provide you with such tracing capabilities.\nSince service meshes intercept all traffic coming in and out of a Pod they can easily do that by just logging each request and providing that information to you.</p>\n<h4>System traces</h4>\n<p>Kubernetes has native support for request tracing for the API server.\nIf enabled it will emit OpenTelemetry events for each request to the API server.</p>\n<p>For more info on how to enable it and use it, check out the <a href=\"https://kubernetes.io/docs/concepts/cluster-administration/system-traces/\">docs</a>.</p>\n<h4>Third-party product</h4>\n<p>A lot of security products also provide some kind of tracing functionality.</p>\n<p>Such is the open-source <a href=\"https://sysdig.com/opensource/inspect/\">SysDig Inspect</a>, which combined with the <a href=\"https://sysdig.com/blog/tracing-in-kubernetes-kubectl-capture-plugin/\">kubectl plugin for tracing</a> allows you to add tracing to your Kubernetes cluster.</p>\n<h3>Highly available control plane components</h3>\n<p>A highly available control plane means that the control plane is running across multiple nodes, with each of the control plane components replicated on at least three nodes.\nThis is so that even if a node fails, the other ones would still provide the functionality, and the cluster would not just stop running.</p>\n<p>This is how all production-grade clusters should be run.\nRunning just a single control plane node is just asking for trouble.\nYou must be aware that infrastructure can and will fail you, so you must be ready when this happens.</p>\n<p>If you are running managed Kubernetes (e.g., <a href=\"https://aws.amazon.com/eks/\">EKS</a>, <a href=\"https://azure.microsoft.com/en-us/services/kubernetes-service/\">AKS</a>, <a href=\"https://cloud.google.com/kubernetes-engine\">GKE</a>, etc.) the cloud provider would probably not allow you to create a single node cluster.\nIf you are managing your own cluster, tools like <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\"><code>kubeadm</code></a> provide you the functionality of <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/\">setting up a highly-available cluster</a>.</p>\n<h3>Certificate management in K8s control plane</h3>\n<p>Most of the internal Kubernetes communication (between the cluster components) is done over HTTPS.\nThis means it requires CA certificates.</p>\n<p>By default, Kubernetes will automatically generate self-signed certificates, which will be used for communication between the components.</p>\n<p>You can also use your own certificates.\nThis can be done by configuring the API server not to generate certificates but instead fetch them from a location on disk.\nTools like <a href=\"https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\"><code>kubeadm</code></a> make this configuration easy.</p>\n<p>You can find more on how to achieve this in <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/\">this article</a>.</p>\n<p>Other tools like <a href=\"/2022/06/12/demystifying-the-kubernetes-iceberg-5/#cert-manager\"><code>cert-manager</code></a> provide integration with CA authorities to automatically fetch and generate new certificates when the old ones expire.</p>\n<h3>node-local-dns</h3>\n<p><a href=\"https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/\">NodeLocal DNS Cache</a> is a DNS cache that aims to reduce the number of times a Pod queries a control plane node to resolve a DNS.\nWith NodeLocal DNS Cache (that runs on each Node as a DaemonSet), the Pods will query the cache for DNS records and only call the API server on a cache miss.</p>\n<p>For more information on how to configure it, check out <a href=\"https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/\">this page</a>.</p>\n<h3>Changes monitoring</h3>\n<p>If you want to continuously monitor the changes of your Kubernetes resource, you can do with <code>kubectl</code> by providing the <code>--watch</code> flag.\nFor example, <code>kubectl get pods --watch</code> will return all Pods in the <code>default</code> namespace, but it will keep the command alive and continue polling the API server. Each time a new pod is created or destroyed, it will output it into the console.</p>\n<p>The same behavior can be implemented via the Kubernetes Go library:</p>\n<pre><code class=\"language-go\">watch, err := client.Client.CoreV1().Pods(&quot;default&quot;).Watch(corev1.ListOptions{})\nif err != nil {\n  // handle error\n}\n\nfor event := range watch.ResultChan() {\n  // use event\n}\n</code></pre>\n<p>This way, you can implement custom dashboards that will help you monitor your Kubernetes cluster.</p>\n<h3>Operators</h3>\n<p>An <a href=\"https://kubernetes.io/docs/concepts/extend-kubernetes/operator/\">operator</a> is a custom Kubernetes controller that interacts with the Kubernetes API to listen for a given object&#39;s creation/update/deletion and trigger some logic based on this event.</p>\n<p>It aims to simplify automation and allow the extensibility of Kubernetes by providing an easy way to implement custom workflows.</p>\n<p>Usually, operators are tied to a <a href=\"/post/demystifying-the-kubernetes-iceberg-2/#crd\">Custom Resource Definition</a>.\nOperators would listen for the create/update/deletion of a Custom Resource Definition and trigger a workflow when this event occurs.</p>\n<p>There are many use-cases for an operator, but provisioning is the most common.\nLet&#39;s say we want to write an operator that provisions PostgreSQL databases in our Kubernetes cluster.\nFirst, we will need to define a CRD.\nThis CRD will contain the basic configuration of the database that we want to be provisioned for us. Once the CRD is applied, the operator will kick in and provision the DB with the specified parameters.\nThat way, all the PostgreSQL-provisioning-specific logic is encapsulated by the operator, and the only thing we as users care about is specifying the wanted configuration.</p>\n<h3>Configuring kubectl for Remote Access</h3>\n<p><a href=\"https://kubernetes.io/docs/reference/kubectl/\">kubectl</a> is a command-line tool used to interact with Kubernetes.\nIt translates CLI commands like <code>kubectl get pods</code> to an HTTP API call to the Kube API server (in this case, <code>GET &lt;api_server_addr&gt;/api/v1/namespaces/default/pods</code>) and outputs the results in the terminal.</p>\n<p><code>kubectl</code> can be used to contact any Kubernetes cluster - local or remote.\n<code>kubectl</code> knows which cluster to contact based on the kubeconfig file.\nA kubeconfig file is a YAML file containing Kubernetes clusters, their addresses, certificates, etc. a kubeconfig file also shows the currently selected cluster.\nTo see the currently selected cluster use the <code>kubectl config current-context</code> command.\nTo see all clusters in your current kubeconfig file, use the <code>kubectl config get-clusters</code> command.\nA context is a combination between a cluster and a user.\nYou can have two contexts for the same cluster but different users.</p>\n<p>You can also run a kubectl command against a different kubeconfig file by using the <code>--kubeconfig</code> flag as described <a href=\"https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/\">here</a></p>\n<h3>Non-destructive update applier</h3>\n<p>After we have created our Kubernetes resource, we often find the need to update them.\nFor example, to change the container image of a Deployment when we release a new version of our application, to reconfigure a Service, etc.</p>\n<p>We can easily do that with the <code>kubectl apply</code> command.\nFor example, we can create our deployment via the <code>kubectl apply -f deployment.yaml</code> command, and later update it via updating the file and again running <code>kubectl apply -f deployment.yaml</code>.</p>\n<p>When calculating the difference between the new and the old spec Kubernetes uses the <a href=\"https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#notes-on-the-strategic-merge-patch\">strategic merge patch approach</a>.\nThis means that Kubernetes tries to be non-destructive - to add all new properties that have been added in the new spec without removing any of the old ones.</p>\n<p>This means that if you want to remove properties from the object, you have to explicitly set them to <code>null</code> in the new spec, or in the case of arrays, use <a href=\"https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/#use-a-json-merge-patch-to-update-a-deployment\">merge directives</a>.</p>\n<h2>Summary</h2>\n<p>This is all for part six.</p>\n<p>We are almost at the bottom of the iceberg now.\nJust two more layers left until we complete it.\nThank you for sharing this journey with me.</p>\n<p>The series continues with <a href=\"/post/demystifying-the-kubernetes-iceberg-7/\">Part 7</a>.</p>\n<p>If you don’t want to miss it, you can follow me on <a href=\"https://twitter.com/a_sankov\">Twitter</a> or <a href=\"https://www.linkedin.com/in/asankov/\">LinkedIn</a>.</p>\n"
            }
        };
    </script>
</body>
</html>
                